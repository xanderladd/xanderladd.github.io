<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-27T23:55:21-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alexander Ladd</title><subtitle>An amazing website.</subtitle><author><name>Alexander Ladd</name><email>zladd@berkeley.edu</email></author><entry><title type="html">Review 20: Trends in Computational Neuroscience: Cosyne 2022</title><link href="http://localhost:4000/reviews/review20/" rel="alternate" type="text/html" title="Review 20: Trends in Computational Neuroscience: Cosyne 2022" /><published>2022-03-25T05:16:13-07:00</published><updated>2022-03-25T05:16:13-07:00</updated><id>http://localhost:4000/reviews/review20</id><content type="html" xml:base="http://localhost:4000/reviews/review20/">&lt;p&gt;&lt;a href=&quot;https://saberatalukder.com/cosyne_2022_computational_and_systems_neuroscience_in_review.html&quot;&gt;Trends in Computational Neuroscience: Cosyne 2022
&lt;/a&gt; by &lt;a href=&quot;https://saberatalukder.com/index.html&quot;&gt;Sabera Talukder&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I couldn’t go to Cosyne but it’s a workshop/conference I’d like to go to one day so I wanted to get the inside scoop of what gets discussed.&lt;/li&gt;
  &lt;li&gt;Fortunately, this blog kept me in the loop, so that’s pretty cool.&lt;/li&gt;
  &lt;li&gt;Notes:&lt;/li&gt;
  &lt;li&gt;Major Trends
    &lt;ul&gt;
      &lt;li&gt;Behavior
        &lt;ul&gt;
          &lt;li&gt;Especially w.r.t. software, data analysis pipelines and pose tracking estimation.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Comparing ANNs to RNNs
        &lt;ul&gt;
          &lt;li&gt;I didn’t have FOMO hearing about behavior, but now I do.&lt;/li&gt;
          &lt;li&gt;One-to-one biological to neural network comparison.&lt;/li&gt;
          &lt;li&gt;Generative modeling neural data with latent representation.
            &lt;ul&gt;
              &lt;li&gt;This one has important impact on neuroengineering.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Biological rules with ML tasks.
            &lt;ul&gt;
              &lt;li&gt;This is one is interesting because I’d like to see how well biologically realistic credit assignment matches up against SOTA methods for MNIST.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;network similarity: how can you tell the distance between two nets?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dimensionality Reduction
        &lt;ul&gt;
          &lt;li&gt;This one trend shows that PCA, UMAP, and tSNE can be improved upon to get low dim. representations that do not warp local and/or global distance. Ah, the curse of dimensionality.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Data Scaling
        &lt;ul&gt;
          &lt;li&gt;How to curate / record / assay large experiments for high throughput data?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;I liked the writing / blog style because it was interesting to read and broke the monotony from reading papers.&lt;/li&gt;
  &lt;li&gt;It was also a concise read so I could read the whole thing instead of skimming. Also packed with a bunch of info about applications of key themes.&lt;/li&gt;
  &lt;li&gt;To me, it’s really exciting to see a conference where ANNs are compared to biological NNs because I find the two topics: generative modeling and biological nets on ML tasks to be interesting because of their relation to eachother. Both tasks seem to be two sides of the neural network coin. For the generative modeling task we are applying nets to better understand the brain and in the ML task we are trying to borrow the brain to make better ML models.&lt;/li&gt;
  &lt;li&gt;Can’t wait to see how these topics unfold in future conferences since I think they’ll have important impact on brain computer interfaces and autonomous control in robotics.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Trends in Computational Neuroscience: Cosyne 2022 by Sabera Talukder</summary></entry><entry><title type="html">Review 19: Let us never speak of these values again.</title><link href="http://localhost:4000/reviews/review19/" rel="alternate" type="text/html" title="Review 19: Let us never speak of these values again." /><published>2022-03-20T05:16:13-07:00</published><updated>2022-03-20T05:16:13-07:00</updated><id>http://localhost:4000/reviews/review19</id><content type="html" xml:base="http://localhost:4000/reviews/review19/">&lt;p&gt;&lt;a href=&quot;https://www.argmin.net/2022/02/23/standard-errors/&quot;&gt;Let us never speak of these values again.&lt;/a&gt; by &lt;a href=&quot;https://www.argmin.net/&quot;&gt;Ben Recht&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This blog covers the utiliy of statistical signifigance measures like effect size and p value.&lt;/li&gt;
  &lt;li&gt;The range of success of claims is predicated off of a ratio of effect size to standard error.&lt;/li&gt;
  &lt;li&gt;Exposes that fact that signifigance is weighted by the spread of probability denisty. Meaning that if there is a slightly favorable outcome with a small PDF and one with a more favorable average outcome but larger PDF, the former may be favored by the p-value.&lt;/li&gt;
  &lt;li&gt;Simple approximations can distort this p-value. Esp. when it comes to averageing across groups.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Most practicing scientists would be better off not knowing what a p-value is.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;This leads to the philosophical problem here: how can we really trust the effect size is valid. This is made challenging by varying levels of validity:
    &lt;ul&gt;
      &lt;li&gt;study design is valid?&lt;/li&gt;
      &lt;li&gt;hypothesis testing is valid?&lt;/li&gt;
      &lt;li&gt;claims are valid?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;I think the third bullet is especially hard. How can you best gurantee performance on an unsen input/output?&lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Let us never speak of these values again. by Ben Recht</summary></entry><entry><title type="html">Review 18: Deep Learning Is Hitting a Wall</title><link href="http://localhost:4000/reviews/review18/" rel="alternate" type="text/html" title="Review 18: Deep Learning Is Hitting a Wall" /><published>2022-03-13T05:16:13-07:00</published><updated>2022-03-13T05:16:13-07:00</updated><id>http://localhost:4000/reviews/review18</id><content type="html" xml:base="http://localhost:4000/reviews/review18/">&lt;p&gt;&lt;a href=&quot;https://nautil.us/deep-learning-is-hitting-a-wall-14467/&quot;&gt;Deep Learning is Hitting a Wall&lt;/a&gt; by &lt;a href=&quot;https://en.wikipedia.org/wiki/Gary_Marcus&quot;&gt;Gary Marcus&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Marcus, Gary. “Deep Learning is Hitting a Wall” natuil.us (2022).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I’ve been writing all day and I am struggling, but reading this is a breath of fresh air. It is well written and easy to read.&lt;/li&gt;
  &lt;li&gt;Marcus claims the AI/ML hype from 2005-2016 has overpromised.&lt;/li&gt;
  &lt;li&gt;We can see it is deployed all the time in low risk and mundane tasks, but we don’t see it often deployed in high risk situations.
    &lt;ul&gt;
      &lt;li&gt;Self-driving being the mos ubiqitiously deployed ML but it requires human oversight.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Large ML language models show  examples of failing to form any underlying conceptual understanding of what the words in the prompt/generation structure mean.&lt;/li&gt;
  &lt;li&gt;Many have touted ML scaling laws, stating that more data and larger models with continue to improve.
    &lt;ul&gt;
      &lt;li&gt;This improvement may be bounded.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Explicit symbol manipulation has been benished from the kingdom of AI.&lt;/li&gt;
  &lt;li&gt;I like that the author focuses on trustworthiness and relibility models instead of accuracy. At the end of the day, humans don’t care about RMSE, we care about the impact it has on  our lives.&lt;/li&gt;
  &lt;li&gt;Background on symbolic AI as codes to represent information. Like bits for example.
    &lt;ul&gt;
      &lt;li&gt;More background about symbol manipulation through explaining computer programs, variable assignment etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Google search as a great example of effective symbolic AI.&lt;/li&gt;
  &lt;li&gt;The autor discuss the background of deep learning rise to fame over symbolic AI involving several key points in the 80s-2010s where certain leading researchers eschewed statements and made claims about which approach has the most merits.&lt;/li&gt;
  &lt;li&gt;Some motivations for moving back to some ideas in symbol based AI:
    &lt;ul&gt;
      &lt;li&gt;Recipe argument: so many things we do are procedural, conditioned. Symbolic AI can better represent this knowledge.&lt;/li&gt;
      &lt;li&gt;Black box: this is the famous DL is a black box argument. I think of this argument as technical debt. You build something capable of doing great things but can’t explicitly explain why it works.&lt;/li&gt;
      &lt;li&gt;Silo arguement: most of the AI capabilities from image detection to NLP are siloed to their own domain. Integrating knowledge from mutiple domains can lead to more general forms of intelligence.&lt;/li&gt;
      &lt;li&gt;Neural networks can’t do addition.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;I think these arguments are well reseasoned and I don’t eschew symbolic AI by any means. However, the question in my mind, is how much symbolism from symbolic AI needs to be introduced.
    &lt;blockquote&gt;
      &lt;p&gt;qtd.  Marcus 2022&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Artur Garcez and Luis Lamb wrote a manifesto for hybrid models in 2009, called Neural-Symbolic Cognitive Reasoning.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Added to my reading list&lt;/li&gt;
  &lt;li&gt;New developments in Symbolic AI
    &lt;ul&gt;
      &lt;li&gt;Alphafold&lt;/li&gt;
      &lt;li&gt;AlphaGO&lt;/li&gt;
      &lt;li&gt;Deepmind chess&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cognition and intelligence is many things and trying to fit them all into feedfoward net w/ backprop is not rational.&lt;/li&gt;
  &lt;li&gt;End with quote:&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;qtd.  Marcus 2022&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;With all the challenges in ethics and computation, and the knowledge needed from fields like linguistics, psychology, anthropology, and neuroscience, and not just mathematics and computer science, it will take a village to raise to an AI. We should never forget that the human brain is perhaps the most complicated system in the known universe; if we are to build something roughly its equal, open-hearted collaboration will be key&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Deep Learning is Hitting a Wall by Gary Marcus</summary></entry><entry><title type="html">Review 17: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 3</title><link href="http://localhost:4000/reviews/review17/" rel="alternate" type="text/html" title="Review 17: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 3" /><published>2022-03-04T04:16:13-08:00</published><updated>2022-03-04T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review17</id><content type="html" xml:base="http://localhost:4000/reviews/review17/">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.07464.pdf&quot;&gt;Predictive Coding, Variational Autoencoders,
and Biological Connections&lt;/a&gt; by &lt;a href=&quot;https://joelouismarino.github.io/&quot;&gt;Joseph Marino&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Marino, Joseph. “Predictive coding, variational autoencoders, and biological connections.” Neural Computation 34.1 (2021): 1-44.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Predictive Coding Continued
    &lt;ul&gt;
      &lt;li&gt;A major difference between predictive coding and inference is that predictive coding calculates gradients with backwards (top-down) projections and inference performs updates bottom up (forward pass).&lt;/li&gt;
      &lt;li&gt;Referring to the previous set of note, these learning updates are computed using:
   \(\nabla_{z}\ell(z, \theta) = W^T (\frac{x - W_z }{\sigma_x}) (\frac{x - \mu_z }{\sigma_z})\)&lt;/li&gt;
      &lt;li&gt;The first team represents a local error $\mathcal{e}_x$ and the second term represents and error over the latent variable $\mathcal{e}_z$. These terms modulate the gain of each error. The paper mentions that they are related to attention and cites Feldman &amp;amp; Friston, 2010. Without any background, I can’t see obviously how the error over the latent variable $z$ could be related to attention.&lt;/li&gt;
      &lt;li&gt;Can expand the preivous equations beyond $\sigma_x$ to be multivariate spatial covariance $\Sigma_X$.&lt;/li&gt;
      &lt;li&gt;Empirical support for predictive coding.
        &lt;ul&gt;
          &lt;li&gt;Fitting retinal cell ganglion cells processes using spatial whitening kernel.&lt;/li&gt;
          &lt;li&gt;Hierachial: top down signals can alter traditional receptive fields.&lt;/li&gt;
          &lt;li&gt;Sensory cortex as engaged in hierarchical prediction of spatial / temporal domain.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variational Auteoencoders
    &lt;ul&gt;
      &lt;li&gt;Deep network latent variable models trained using variational inference.
        &lt;ul&gt;
          &lt;li&gt;Breaking this down:
            &lt;ul&gt;
              &lt;li&gt;deep network - stacked layers of units / artifical neurons that perform linear transformations to the signal as it feeds forward.&lt;/li&gt;
              &lt;li&gt;latent variable -  some random variables (noramlly denoted as $z$) which will be part of the model parameterizaiton. To me, an interesting intuition to thinking about latent variables is that they create a low dimensional subspace that bottlenecks the information from higher dimensional inputs / layers.&lt;/li&gt;
              &lt;li&gt;using variational inference - this one was covered in the previous blog.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Optimizing conditional likelihood of latent variable requires differentiating the latent variable. This is tricky because the latent variable is sampled from a  normal distribution. How can we tune the parameters of this distribution &lt;em&gt;and&lt;/em&gt; the network? To do this we need to use the reparameterization trick. For brevity I am not going to cover in this blog. This allows us to train a network that gives latent variable vector $\mathbf{z}$, thus an autoencoder.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Connections and Comparisions
    &lt;ul&gt;
      &lt;li&gt;Main connection: both are hierachcial latent variable models.
        &lt;ul&gt;
          &lt;li&gt;Also, both preditive coding and VAEs rely on nonlinear connections between levels and covariance relationship between levels.
            &lt;ul&gt;
              &lt;li&gt;IMO, this is the really impressive part about VAE and why it’s so relevant for Neuroscience. It is well known that human NNs have nonlinear synaptic connectivty, but I think it’s less clear (at least for the layman) that levels of cortical connectiviy might covary. But we know, especically from researching attention, that this is true. This is why VAE’s are becoming my favorite brain inspired ML model! Although, credit assignment is still an outstanding issue.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Inference: both methods perform inference but using diffeent optimization techniques (predictive: gradient, vae: amortized). (see last two reviews and previous section about optimizing VAE)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Author aligns VAE and cortical network on the level of a network of pyramidal neuron dendrites.
        &lt;ul&gt;
          &lt;li&gt;I don’t know why this alignment is against dendrites. Why not against the entire neuron (axon, soma, etc.)&lt;/li&gt;
          &lt;li&gt;Ok acutally I do think I know why: it’s because these networks mainly integrate signal. Or at least it’s easier to make the comparison they are integrating signal rather than integrating the signal and propogating it.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Normalizing flows, which is mentioned in the paper, but not this blog, is compared to lateral neurons / lateral inhibition.
        &lt;ul&gt;
          &lt;li&gt;I am generally making this note for myself to remember this.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Predictive Coding, Variational Autoencoders, and Biological Connections by Joseph Marino</summary></entry><entry><title type="html">Review 16: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 2</title><link href="http://localhost:4000/reviews/review16/" rel="alternate" type="text/html" title="Review 16: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 2" /><published>2022-02-26T04:16:13-08:00</published><updated>2022-02-26T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review16</id><content type="html" xml:base="http://localhost:4000/reviews/review16/">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.07464.pdf&quot;&gt;Predictive Coding, Variational Autoencoders,
and Biological Connections&lt;/a&gt; by &lt;a href=&quot;https://joelouismarino.github.io/&quot;&gt;Joseph Marino&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Marino, Joseph. “Predictive coding, variational autoencoders, and biological connections.” Neural Computation 34.1 (2021): 1-44.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Predictive encoding from the perspective of theoretical neuroscience and machine learning.&lt;/li&gt;
  &lt;li&gt;2.3 Variational Inference
    &lt;ul&gt;
      &lt;li&gt;Previously paper introduces $p_{\theta}(x,\mathbf{z})p_{\theta}(x)$ as latent variable models.&lt;/li&gt;
      &lt;li&gt;To get $p_{\theta}(x) = \int p_{\theta}(x,\mathbf{z})d\mathbf{z}$ is often an intractable integral.&lt;/li&gt;
      &lt;li&gt;Variational inference using evidence lower bound optimization (ELBO) $ \mathcal{L}(\mathbf{x}; q, \theta) \leq log p_{\theta}(\mathbf{x})$.&lt;/li&gt;
      &lt;li&gt;ELBO objective:
  \(log p_{\theta}(\mathbf{x} = \mathcal{L}(\mathbf{x}; z, \theta) + KL(q(\mathbf{z} \mid \mathbf{x}) || p_{\theta}(\mathbf{z} \mid \mathbf{x})  )\)&lt;/li&gt;
      &lt;li&gt;Apporaches to optimizing:
        &lt;ul&gt;
          &lt;li&gt;Alternating minimization (EM) between first and second term of ELBO.&lt;/li&gt;
          &lt;li&gt;Computatiional graphn and differentiaion.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Predictive Coding
    &lt;ul&gt;
      &lt;li&gt;Spatiotemporal
        &lt;ul&gt;
          &lt;li&gt;Gaussian autoregessive model over sequences $x_1 … x_t$:
  \(p(x_t \mid x_{1:T}) = \mathcal{N}(x_t; \mu_{x_{\leq t}}, diag(\sigma_{x_{\leq t}^2}))\)
            &lt;ul&gt;
              &lt;li&gt;introduce $y_t \sim \mathcal(0,I) $ as prediction error: $ x_t =  \mu_{x_{\leq t}} + \sigma_{x_{\leq t}^2} \cdot y_t$&lt;/li&gt;
              &lt;li&gt;whitenting transformation shows how this is actually prediction eror: $ y_t = \frac{x_t - \mu_{x_{\leq t}}}{\sigma_{\theta}(x_{\leq t})} $&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Prediticiton error can be measured over spatial dimensions using  autoregressive transform.
            &lt;ul&gt;
              &lt;li&gt;spatial whitening formula $\mathbf{y} = \Sigma_{\theta}^{-1/2}(x - \mathbf{\mu_{\theta}}) $&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Example of predictive coding: fly optic nerve performs autocorrelation of spatiotemporal signals for dyanmic range.&lt;/li&gt;
          &lt;li&gt;Normalization through inhibitory neurons.&lt;/li&gt;
          &lt;li&gt;Photorecptor inhibition to filter unpredicted motion. Ie; object was still in the background but now it is moving.&lt;/li&gt;
          &lt;li&gt;Lateral inihibhition for spatiotemporal normalization.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Hierarchical
        &lt;ul&gt;
          &lt;li&gt;Six layers of neocortex (I - VI)  organized into columns that communicate through interneurons and long range connections vis pyramidal neurons.&lt;/li&gt;
          &lt;li&gt;Neocortical microcircuit w/ forward / backwards, excitatory / inihbitory projections.&lt;/li&gt;
          &lt;li&gt;Theory: backward projections contain predictions and foward projections have error signal.&lt;/li&gt;
          &lt;li&gt;Mathematical formulation of thalamus as an active blackboard that aims to minimize prediction error by Rao &amp;amp; Ballard (1999).
            &lt;ul&gt;
              &lt;li&gt;$p_{\theta}(x \ mid z) = \mathcal{N}(x; f(Wz), diag(\sigma^2_{x}))$&lt;/li&gt;
              &lt;li&gt;$p_{\theta}(z) = \mathcal{N}(x; \mu_{z}, diag(\sigma^2_{z}))$
                &lt;ul&gt;
                  &lt;li&gt;f = elementwise function, w = weight matrix, $\sigma_{x}^2$ and $\sigma_{z}^2$ are the respective variances for $x$ and $z$.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;MAP estimate of z is $z^*$ which maximizes $p_{\theta}(z \mid x)$.&lt;/li&gt;
              &lt;li&gt;Formulate that as an optimization problem: \(z^* = argmax_{z} ( log(p_{\theta}(x \mid z)) + log p_{\theta}(z) )\)&lt;/li&gt;
              &lt;li&gt;Each term in the above is convex squared error term so the whole thing can be solved analytically with \(\nabla_{z}\ell(z, \theta) = W^T (\frac{x - W_z }{\sigma_x}) (\frac{x - \mu_z }{\sigma_z})\)
                &lt;ul&gt;
                  &lt;li&gt;We can also use the above gradient to solve for $W$ w/ $\nabla_{W}$&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Under this construction of $\theta(W,z)$ we get learning rules that aim to be biologically plausible by optimizing against physical occurrances, like firing rate or membrane potential. Extending this, the latent variable model (LVM) can be the particular output of a specific cortical column.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Various forms of evidence for this model in different sensory areas of mice, but ultimately, predictive coding is an incomplete theory because most of these models are oversimplified circuit models.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Predictive Coding, Variational Autoencoders, and Biological Connections by Joseph Marino</summary></entry><entry><title type="html">Review 15: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 1</title><link href="http://localhost:4000/reviews/review15/" rel="alternate" type="text/html" title="Review 15: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 1" /><published>2022-02-18T04:16:13-08:00</published><updated>2022-02-18T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review15</id><content type="html" xml:base="http://localhost:4000/reviews/review15/">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.07464.pdf&quot;&gt;Predictive Coding, Variational Autoencoders,
and Biological Connections&lt;/a&gt; by &lt;a href=&quot;https://joelouismarino.github.io/&quot;&gt;Joseph Marino&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Marino, Joseph. “Predictive coding, variational autoencoders, and biological connections.” Neural Computation 34.1 (2021): 1-44.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Predictive encoding from the perspective of theoretical neuroscience and machine learning.&lt;/li&gt;
  &lt;li&gt;Informal defintion for predictive encoding: Neural circuits as probabilistic models of other neurons.
    &lt;ul&gt;
      &lt;li&gt;Inception of this idea in cirucits for sensory processing, like the retina and visual pathways.&lt;/li&gt;
      &lt;li&gt;Feedbackward connections that apply prediction error signal.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.nature.com/articles/nn0199_79&quot;&gt;Rao, Ballard 1999&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;For neuroscience, cybernetics, hemholtz machine, and predictive encoding are the inspiration for Friston 2005 and 2008 work on free energy and active inference.&lt;/li&gt;
  &lt;li&gt;For machine learning, this earlier work in variational inference and encoder-decoder models, culminates in the variational autoencoder.&lt;/li&gt;
  &lt;li&gt;Lots of overlap conceptually, but much of the research between the two fields is divided.&lt;/li&gt;
  &lt;li&gt;The next section is titled connecting predictive coding and VAEs. I take it that these two concepts form the bridge between the fields.
    &lt;ul&gt;
      &lt;li&gt;Just from the intro I like that this paper points two very similar concepts in two different fields and then describes the relationship between the two. I feel that there the’s potential for a lot of discovery in understanding the union and intersection between these two concepts.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Paper posits two possible correspondences between ML and neuro.
    &lt;ul&gt;
      &lt;li&gt;Dendrites of pyramidal neurons and neuronal networks.&lt;/li&gt;
      &lt;li&gt;Lateral inhibition and normalizing flows.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Background info
    &lt;ul&gt;
      &lt;li&gt;MLE
        &lt;ul&gt;
          &lt;li&gt;How can we find some distribution $p_{data}$ using r.v. samples $\bf{x} \sim \hat{p}_{data}(\bf{x})$?&lt;/li&gt;
          &lt;li&gt;Maximizing &lt;strong&gt;log likelihood&lt;/strong&gt; of samples under that distribution. 
  \(\theta^* \longleftarrow argmax_{\theta} \mathbb{E}_{x \sim p_{data}(\bf{x})} [log(p_{theta}(\bf{x}))]\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Probabilistic models
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;autoregressive models&lt;/strong&gt;: $ p_{\theta(\bf{x})} = \prod_{j=1}^{m} p(\bf{x}_{j} \mid x_{&amp;lt; j} $&lt;/li&gt;
          &lt;li&gt;Can factor out $\bf{x}$ in above as a series of distributions, from $t=1 … T$&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Latent variable models (LVMs)&lt;/strong&gt;  with $z$ have the joint distribtion: \(p_{\theta}(\bf{x}, \bf{z}) = p_{theta}(\bf{x} \mid z)p_{theta}(\bf{z})\)&lt;/li&gt;
          &lt;li&gt;$\bf{z}$ is being used here to describe $\bf{x}$, incurs computational cost.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;flow based LVMs&lt;/strong&gt; \(p_{\theta}(\bf{x}) = p_{\theta}(x) \| det(\frac{\partial{\bf{x}}}{\partial{\bf{z}}}) \|^{-1}\)
            &lt;ul&gt;
              &lt;li&gt;Distilling this down to the idea that there if some function $f_{\theta}$ that can transfrom $f_{\theta}(\bf{x}) = \bf{z}, \ f_{\theta}^{-1}(\bf{z}) = \bf{x}$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Can combine the above techniques for &lt;strong&gt;hierachical LVMs&lt;/strong&gt;, &lt;strong&gt;sequential LVMS&lt;/strong&gt; etc.
            &lt;ul&gt;
              &lt;li&gt;An interesting example is stacking latent variables $ \bf{z}^{1:L} = [ \bf{z}_{1} … \bf{z}_{L} ] $ and \(p_{\theta}(\bf{x},\bf{z}^{1:L}) = p_{\theta}(\bf{x}, \bf{z}^{1:L}) \prod_{\ell=1}^{L} p(\bf{z}^{\ell} \mid z^{\ell+1:L})\)&lt;/li&gt;
              &lt;li&gt;I am confused by the conditional probability $ p(\bf{Z^{\ell}} \mid z^{\ell+1:L})$ and the directionality of the hierarchy. Why wouldn’t it go backwards:  $ p(\bf{Z^{\ell}} \mid z^{1:\ell+1})$&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Fitting these models
            &lt;ul&gt;
              &lt;li&gt;Log density of unit normal distribution becomes mean squared error.&lt;/li&gt;
              &lt;li&gt;A simple univariate autoregressive model can be formulated as $ p_{\theta}(x_j \mid x_{&amp;lt; j}) = \mathcal{N}(x_j; \mu_{\theta}(x_{&amp;lt; j}), \sigma^2_{\theta}(x_{&amp;lt; j})) $ .
                &lt;ul&gt;
                  &lt;li&gt;I think there’s a minor notation error here that the $x_{&amp;lt; j}$ should be boldface since it is still a vector.&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Training using the gradient of log likelikehood: $\nabla_{\theta}\mathbb{E}_{\bf{x} \sim p_{data}}[log p_{\theta}(\bf{x})]$&lt;/li&gt;
              &lt;li&gt;Deep autoregressive and variational models can be broken apart by their latent variable impact on the objective with some interesting results.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Variational Inference
        &lt;ul&gt;
          &lt;li&gt;TODO: Left off here section 2.3&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Predictive Coding, Variational Autoencoders, and Biological Connections by Joseph Marino</summary></entry><entry><title type="html">Review 14: Computation Through Neural Population Dynamics Part 2</title><link href="http://localhost:4000/reviews/review14/" rel="alternate" type="text/html" title="Review 14: Computation Through Neural Population Dynamics Part 2" /><published>2022-02-13T04:16:13-08:00</published><updated>2022-02-13T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review14</id><content type="html" xml:base="http://localhost:4000/reviews/review14/">&lt;p&gt;&lt;a href=&quot;https://www.annualreviews.org/doi/pdf/10.1146/annurev-neuro-092619-094115&quot;&gt;Computation Through Neural Population Dynamics&lt;/a&gt; by &lt;a href=&quot;https://smvyas.github.io/&quot;&gt;Saurabh Vyas&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Citation: Vyas, Saurabh, et al. “Computation through neural population dynamics.” Annual Review of Neuroscience 43 (2020): 249-275.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;see &lt;a href=&quot;https://xanderladd.github.io/reviews/review13/&quot;&gt;part 1&lt;/a&gt; if you haven’t.&lt;/li&gt;
  &lt;li&gt;Picking up where I left off: Population Dynamics Underlying Motor Adaptation.
    &lt;ul&gt;
      &lt;li&gt;To shift focus from motor pattern generation to adaption they use a task where cursor movement is offset by a 45 degree counter-clockwise angle and the subject must learn to adapt to the offset of the cursor to get the cursor to the target.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;washout&lt;/strong&gt;: once this adaptation is made, reversing the cursor movement offset causes errors in the opposite direction.&lt;/li&gt;
      &lt;li&gt;Electrical microstimulation during these trials did not affect performance in the trials but afterward. For ML people: stimulation introduces uncertainty so the system reduces its learning rate to temper this uncertainty.&lt;/li&gt;
      &lt;li&gt;Planning, but not execution allowed humans to learn different curl force field (CF) adaptations, like the one described above, as contextual input. Learning does not require dramatic changes to pattern gen. circuitry, but there is a shift between orthogonal subspaces&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Implications of Manifold Structure on Learning
    &lt;ul&gt;
      &lt;li&gt;Hard to casually attribute movements to patterns of neurons, but BCI direct input to movement is helpful.&lt;/li&gt;
      &lt;li&gt;Linear manifold created by BCI movements in unperturbed trials and then add perturbations that require movement outside of this linear manifold. This results in inside/outside intrinsic manifold.&lt;/li&gt;
      &lt;li&gt;Rearranging of prelearned associations to adapt to outside intrinsic manifold.&lt;/li&gt;
      &lt;li&gt;Redefine manifold based on inside/outside prelearned repertoire.&lt;/li&gt;
      &lt;li&gt;RNN modeling approaches show that significant changes to RNN weights are required for out of manifold dynamics.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dynamical Systems in Cognition
    &lt;ul&gt;
      &lt;li&gt;Motor Timing
        &lt;ul&gt;
          &lt;li&gt;State-dependent networks and ramping neurons as time modulation systems.&lt;/li&gt;
          &lt;li&gt;Ready set go tasks show speed modulation in dorsomedial frontal cortex.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Decision Making and Working Memory
        &lt;ul&gt;
          &lt;li&gt;” Context dependence was not a result of traditional gating but rather of dynamics quenching activity along certain directions in state space, achieved via a contextually dependent state space shift.” - Vyas et al. 2020&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;RNN trajectory tunnels force states into stable trajectories.&lt;/li&gt;
      &lt;li&gt;Key insights are that: a) Delayed association in biological circuits can be performed using transient dynamics and b) different regions of state space can subserve different functional roles.&lt;/li&gt;
      &lt;li&gt;the recombination of actions to accomplish a certain task is an important research direction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Challenges and Opportunities&lt;/li&gt;
  &lt;li&gt;Remaining questions:
    &lt;ul&gt;
      &lt;li&gt;Where do inputs come from and how do they affect local circuit dynamics?&lt;/li&gt;
      &lt;li&gt;What is I/O for a specific brain region?&lt;/li&gt;
      &lt;li&gt;How do brain regions affect each other?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optogenetic stimulation as a powerful approach for testing CTD model framework.&lt;/li&gt;
  &lt;li&gt;Three steps:
    &lt;ol&gt;
      &lt;li&gt;Continued development of CTD framework.&lt;/li&gt;
      &lt;li&gt;Further research into methodological approaches towards causal predictions.&lt;/li&gt;
      &lt;li&gt;Providing concrete strategies for updating models.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;The first and last steps are very startup-esque. Build and rebuild kind of thing. The second point is a serious epistemological problem.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Computation Through Neural Population Dynamics by Saurabh Vyas</summary></entry><entry><title type="html">Review 13: Computation Through Neural Population Dynamics Part 1</title><link href="http://localhost:4000/reviews/review13/" rel="alternate" type="text/html" title="Review 13: Computation Through Neural Population Dynamics Part 1" /><published>2022-02-04T04:16:13-08:00</published><updated>2022-02-04T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review13</id><content type="html" xml:base="http://localhost:4000/reviews/review13/">&lt;p&gt;&lt;a href=&quot;https://www.annualreviews.org/doi/pdf/10.1146/annurev-neuro-092619-094115&quot;&gt;Computation Through Neural Population Dynamics&lt;/a&gt; by &lt;a href=&quot;https://smvyas.github.io/&quot;&gt;Saurabh Vyas&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Citation: Vyas, Saurabh, et al. “Computation through neural population dynamics.” Annual Review of Neuroscience 43 (2020): 249-275.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gonna make this a two-part review since this one is a little longer.&lt;/li&gt;
  &lt;li&gt;The dynamic systems approach to population neuron activity is  interesting. I just recently learned about the field called dynamical systems, which is exciting. I had always thought “dynamical system” was an adjective (and it is), but I never imagined they have their own field of mathematical research.
    &lt;ul&gt;
      &lt;li&gt;formal def. of dynamical system: $\frac{dx}{dt} = f(x(t), u(t)) $.&lt;/li&gt;
      &lt;li&gt;$x$ is the neural population response at time $t$ and $u$ is the external input.&lt;/li&gt;
      &lt;li&gt;pendulum as a brief example of a dynamical system.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Move from introduction to the brain as a dynamical system to discussing neuronal population firing trajectory.
    &lt;ul&gt;
      &lt;li&gt;Here they bring up dimensionality reduction as an efficient method for simplifying the case when you have thousands of neuron trajectories and of course this is a valid approach. However, the statement that “we only lose $x$ amount of variance with PCA, so PCA is good” is one I take with a grain of salt. Most data can have most of its variance accounted for by 10 PCs. I don’t think accounted variance means dim. reduction on neural trajectories is a great idea. On the other hand, it’s not likely we’re gonna start looking at all trajectories and find intuitive understanding, so I’ll stop complaining.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Computation through Dynamics (CTD)&lt;/strong&gt; : dynamics of how neural circuit + input evolve through state space.
    &lt;ul&gt;
      &lt;li&gt;underlying all the things we do!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CTD mathematical intro
    &lt;ul&gt;
      &lt;li&gt;Deep Learning
        &lt;ul&gt;
          &lt;li&gt;Data modeling: use the internal state to find $f$&lt;/li&gt;
          &lt;li&gt;Task modeling: input / output pairs from a task and we wish to identify $f$ that could have possibly led to these pairs.
            &lt;ul&gt;
              &lt;li&gt;Task modeling seems so much harder!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;RNNs as the neural network / DL approach to solving the $f$ responsible for dynamical systems.
            &lt;ul&gt;
              &lt;li&gt;glad they used the pendulum example to explain this.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;High Dim. Systems
        &lt;ul&gt;
          &lt;li&gt;Linear dynamical system (LDS) : $\frac{dx}{dt} = L(x(t),u(t)) = Ax(t) + Bu(t)$&lt;/li&gt;
          &lt;li&gt;Linear dynamics around fixed points: attractor, repeller, oscillator.&lt;/li&gt;
          &lt;li&gt;Stable orbits / unstable orbits.&lt;/li&gt;
          &lt;li&gt;LDS around fixed point with input: $\frac{d\delta (s)}{dt} = A(x^*,u^*)\delta x(t) + B(x^*,u^*)\delta u(t) $
            &lt;ul&gt;
              &lt;li&gt;$\delta x(t) = x(t) - x^*$ and same for $\delta u(t)$. This just means they have an initial point denoted by $*$ term.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Linear Subspaces and Manifolds
        &lt;ul&gt;
          &lt;li&gt;2D manifold looks like a pringle.&lt;/li&gt;
          &lt;li&gt;Most of this was reviewed, but I did not understand the purpose of figure 6 showing potent space and null space.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dynamical Systems in Motor Control
    &lt;ul&gt;
      &lt;li&gt;What principles guide the generation of the observed time-varying patterns of motor cortical activity?&lt;/li&gt;
      &lt;li&gt;Delay reaching task as an example of a common experimental paradigm for motor preparation.
        &lt;ul&gt;
          &lt;li&gt;CTD views preparatory activity as an initial condition. Not a necessary condition though; Other initial states can have the same motor movement, but being closer to the right one is advantageous for several reasons that they explain.
            &lt;ul&gt;
              &lt;li&gt;Competing hypothesis presented.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;RNN model to try to produce motor activity given preparatory activity.&lt;/li&gt;
          &lt;li&gt;Null spaces as preparation preclude some activity because it is orthogonal to that initial condition.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dynamical motifs during movement
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Condition invariant signal&lt;/strong&gt;: signal that follows timing/onset of cue. Large component of population response.
            &lt;ul&gt;
              &lt;li&gt;Roughly, releases the preparatory state from an attractor-like state to a rotating state.&lt;/li&gt;
              &lt;li&gt;The paper with citation (Sussile 2015) that is a “sufficiency test for the rotations motif” seems really interesting. Simple low dim RNN oscillation produced complex inputs/outputs for prep state –&amp;gt; muscle activity.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;A really interesting point is that maybe these oscillating states are inevitable outcomes of basic neuronal firing properties.
            &lt;ul&gt;
              &lt;li&gt;I don’t think asking “when are these oscillating states informative?” is as clear as asking “how informative are these oscillation states?”.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Two similar states –&amp;gt; same movement == tangling. Then the reverse process is untangling.&lt;/li&gt;
          &lt;li&gt;Low degree of trajectory divergence in supplementary motor area (SMA).&lt;/li&gt;
          &lt;li&gt;Summart lost me a (d) - (f). Feels like I missed those later points somewhere in the reading.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Computation Through Neural Population Dynamics by Saurabh Vyas</summary></entry><entry><title type="html">Review 12: Just Ask for Generalization</title><link href="http://localhost:4000/reviews/review12/" rel="alternate" type="text/html" title="Review 12: Just Ask for Generalization" /><published>2022-01-29T04:16:13-08:00</published><updated>2022-01-29T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review12</id><content type="html" xml:base="http://localhost:4000/reviews/review12/">&lt;p&gt;&lt;a href=&quot;https://evjang.com/2021/10/23/generalization.html&quot;&gt;Just Ask for Generalization&lt;/a&gt; by &lt;a href=&quot;https://evjang.com/&quot;&gt;Eric Jang&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Jang 2021&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Generalizing to what you want may be easier than optimizing directly for what you want. We might even ask for “consciousness”.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;I read this hook and I was immediately into reading the blog. Saying something like “generalizing for consciousness” is engaging. Although it’s a grand claim from the looks of it, it’s a great way to start by saying &lt;em&gt;something&lt;/em&gt;. I think scientists are so careful about the claims they make, that their hooks become boring even in blogs. This is what blogs are for, let it fly! I’m in.&lt;/li&gt;
  &lt;li&gt;The name “grokking” in the double descent phenomenon is weird. The authors could have chosen such a cooler name.
    &lt;ul&gt;
      &lt;li&gt;This is the idea that if you keep training after training loss has converged you can still get better generalization in test data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Recently, we’ve seen neural networks and ML have a simple narrative. Lots of data, lots of computing, and lots of parameters (high capacity model) can do some great things.
    &lt;ul&gt;
      &lt;li&gt;The blog shows DALL-E which is really an excellent example of generalization in ML.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Interesting transition into the failure of RL to generalize the same way.&lt;/li&gt;
  &lt;li&gt;Setting up Markov Decision Processes (MDP).
    &lt;ul&gt;
      &lt;li&gt;Distribution of actions in states: $p(a \mid s)$, reward: $p(r_t, s_t)$ and transition probabilities $p(s_{t+1} \mid s_t, a_t)$.&lt;/li&gt;
      &lt;li&gt;Goal to maximize $R(\Theta)$ (reward) where theta is a policy over $t_i=1 \longrightarrow t$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In RL we don’t know the optimal policy, so we use the experience of the model.&lt;/li&gt;
  &lt;li&gt;The various ways of doing this involve learning updates with gradients of the expected reward. Can learn by sampling policy parameters (CMA-ES), or by gradient guided actions (PPO).&lt;/li&gt;
  &lt;li&gt;There are lots of sources of variance with RL, like the starting position of the agent, non-determinism in the state action transitions. Using an online learning policy incurs more of this variance and researchers need to have a large minibatch to get stable training outcomes.&lt;/li&gt;
  &lt;li&gt;“Offline RL is not as data absorbent as supervised learning”&lt;/li&gt;
  &lt;li&gt;Decision Transformer as an example of generalization in predicting all possible policy trajectories (not just the good ones)&lt;/li&gt;
  &lt;li&gt;There is a lot that can be learned from simply differentiating good from bad policies and D-REX aims to leverage a perturbation kind of approach towards this.&lt;/li&gt;
  &lt;li&gt;The move to begin making conclusions starts with this really cool “just ask for generalization” table.
    &lt;ul&gt;
      &lt;li&gt;Provides the “generalized” version of each RL optimization problem.&lt;/li&gt;
      &lt;li&gt;There are a few, but “watch try learn” is the most interesting to me. The idea of learning a function that learns &lt;em&gt;how learning policies are learned&lt;/em&gt; seems inefficient at first look but is actually very useful.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To sum some of this up: the generalized approaches change the single trajectory from initial states to one optimal policy to initial states to multiple trajectories.&lt;/li&gt;
  &lt;li&gt;Now blog circles back to that initial flashy point on consciousness.
    &lt;ul&gt;
      &lt;li&gt;Author recommends reading the recipe for consciouesness with a few strong drinks but I’m hung over so a beer will have to do.&lt;/li&gt;
      &lt;li&gt;(roughly) the idea is that by modeling how other agents imitate eachother using optimal / suboptimal policies will require agents to introspect into other agents and this will create a more convincing form of conscious agent.&lt;/li&gt;
      &lt;li&gt;Can a policy recognize itself? I think this is a fascinating question and another step into a unqiue type of machine intelligence that must construct a detailed intermediate of representations of the world through the state space/policy optimization approach.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Just Ask for Generalization by Eric Jang</summary></entry><entry><title type="html">Review 10: Beyond advertising: New infrastructures for publishing integrated research objects</title><link href="http://localhost:4000/reviews/review10/" rel="alternate" type="text/html" title="Review 10: Beyond advertising: New infrastructures for publishing integrated research objects" /><published>2022-01-16T04:16:13-08:00</published><updated>2022-01-16T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review10</id><content type="html" xml:base="http://localhost:4000/reviews/review10/">&lt;p&gt;&lt;a href=&quot;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009651&quot;&gt;Beyond advertising: New infrastructures for publishing integrated research objects&lt;/a&gt; by &lt;a href=&quot;https://elizabeth-dupre.com/#/cv&quot;&gt;Elizabeth DuPre&lt;/a&gt; et al.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Citation: DuPre, Elizabeth, et al. “Beyond advertising: New infrastructures for publishing integrated research objects.” PLOS Computational Biology 18.1 (2022): e1009651.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Introduction
    &lt;ul&gt;
      &lt;li&gt;Discuss current practices in reviewing and publishing a paper, including new forms of curation like openreview and arxivsantiy. Also includes new practices like website hosting and Twitter paper advertising.&lt;/li&gt;
      &lt;li&gt;Presents problem: Researchers have moved online but have not fully embraced web first workflows.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hybrid Objects
    &lt;ul&gt;
      &lt;li&gt;Hybrid objects are described as an amalgamation of writing, code, computation, data, and documentation.&lt;/li&gt;
      &lt;li&gt;The authors point out the major problem with hybrid objects, at least in regards to the data, which is a lack of data standards in most data publishing platforms.
        &lt;ul&gt;
          &lt;li&gt;And who can blame these data hosting platforms? If they want people to use them, they need looser data standards than their competitors. On the other hand, enforcing and complying to data standards is &lt;em&gt;huge&lt;/em&gt; problem because there are so many different possible formats to have data in (.json, .csv, .dat, .hdf5, .yaml, and the worst: .txt)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;However, following the most prevalent compliant standard for publishing data sets should aid prospects of publication.
        &lt;ul&gt;
          &lt;li&gt;yes this is quite literally conformist.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;When it comes to data standards having an ecosystem is a very bad thing&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;I think the best way to implement data standards is inheritance. You can make instantiations of some broader standard.&lt;/li&gt;
          &lt;li&gt;Though, this problem is a rabbit hole.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Interactive and integrated research objects
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Claerbout Challenge&lt;/strong&gt; is the problem of sharing your whole code/data and env. It always comes with caveats like dependencies that take forever to install.
        &lt;ul&gt;
          &lt;li&gt;“Code provided for reproducibility” is often not as reproducible as we’d like it to be.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Sharing integrated research objects like notebooks is nice approach for this.&lt;/li&gt;
      &lt;li&gt;But then comes the challenge of hosting such objects. So if you make a notebook, where do you put it so that people can engage. (I think github is fine).&lt;/li&gt;
      &lt;li&gt;But this eLife pilot executable research article looks awesome!
        &lt;ul&gt;
          &lt;li&gt;did not know that existed.
            &lt;blockquote&gt;
              &lt;p&gt;DuPre 2022&lt;/p&gt;
              &lt;blockquote&gt;
                &lt;p&gt;We argue that sustainable development demands open standards with multistakeholder governance and leadership to ensure that resulting specifications are not driven by a single stakeholder.&lt;/p&gt;
              &lt;/blockquote&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;This is word soup, but I totally agree with the sentiment. Open standards need to be equitable for all researchers. Python notebooks are great if you use python, but if you only know matlab or R, these formats could cause you a great deal of trouble.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Combining RMarkdown and Jupyter Notebooks seems like an excellent way to present research.
    &lt;ul&gt;
      &lt;li&gt;Though I might sub out RMarkdown for plain old markdown, or even better Jupyter Notebooks native mathjax (basically latex).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dupre 2022&lt;/p&gt;
  &lt;blockquote&gt;
    &lt;p&gt;By investing in infrastructure for integrated research objects that heavily rely on open, modular components, we can make strong contributions in individual research domains while still ensuring that these investments can be easily retooled and extended.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;This is a very good sentiment to have for these platforms. Building such components to be equitable across large and small research groups is a difficult challenge as the authors have pointed out. But yes, &lt;a href=&quot;https://www.neurolibre.org/&quot;&gt;NeuroLibre&lt;/a&gt; and &lt;a href=&quot;https://pangeo.io/&quot;&gt;Pangeo&lt;/a&gt; are steps in the right direction.&lt;/li&gt;
  &lt;li&gt;My only real opinion here is that it doesn’t seem to help a paper if it has these integrated objects. I don’t see many reviewers taking this stuff into account, but there should be an incentive for both authors and reviewers to do so. I think this would create more of a community around these tools and then things can build from there. The problem right now is that it is too attractive to publish and then never touch your code again.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Beyond advertising: New infrastructures for publishing integrated research objects by Elizabeth DuPre et al.</summary></entry></feed>