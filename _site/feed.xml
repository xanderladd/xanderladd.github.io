<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-09T12:52:49-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alexander Ladd</title><subtitle>An amazing website.</subtitle><author><name>Alexander Ladd</name><email>zladd@berkeley.edu</email></author><entry><title type="html">Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</title><link href="http://localhost:4000/reviews/review9/" rel="alternate" type="text/html" title="Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning" /><published>2022-01-09T04:16:13-08:00</published><updated>2022-01-09T04:16:13-08:00</updated><id>http://localhost:4000/reviews/review9</id><content type="html" xml:base="http://localhost:4000/reviews/review9/">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2109.02869.pdf&quot;&gt;The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning&lt;/a&gt; by &lt;a href=&quot;https://otoro.net/ml/&quot;&gt;Dr. Ha&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/yujin-tang-98b3ab5a/?originalSubdomain=jp&quot;&gt;Yujin Tang&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Citation: Tang, Yujin, and David Ha. “The sensory neuron as a transformer: Permutation-invariant neural networks for reinforcement learning.” Advances in Neural Information Processing Systems 34 (2021).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Today, I am reading a paper in NeurIPS by Dr. David Ha and Dr. Yujin Tang.&lt;/li&gt;
  &lt;li&gt;These reviews are not really reviews but more like casual reading notes. I am writing what pops into my head as I read.&lt;/li&gt;
  &lt;li&gt;The first thing that stands out to me just from reading the introduction is that shuffling around sensory input is generally fatal for most network problems. I doubt most image recognition modules would work at all if one just shuffled around 4x4 pixel blocks in an image.
    &lt;ul&gt;
      &lt;li&gt;I can think of a few problems that begin to crop up here.&lt;/li&gt;
      &lt;li&gt;Firstly, if we think about swapping the pixels in a picture of a cat one by one, how many swaps do we need to make until it is just not a picture of a cat.
        &lt;ul&gt;
          &lt;li&gt;This issue is rectified in the paper because they are trying to complete a task or play a game, which means the objective is still definitively the same regardless of input shuffling.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Secondly, how does one parameterize image shuffling? Say we decide to shuffle around $64 x 64$ squares in an image. If we know the image is $256 x 256$ then it makes sense to define $16$ networks. But we’ve cheated and used some a priori knowledge about the image permutations.
        &lt;ul&gt;
          &lt;li&gt;$\frac{256 \times 256}{64 \times 64} = 16$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;But now I start getting ahead of myself, I’ll keep reading and perhaps these issues aren’t relevant. Onward!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;One more thing in the introduction is really surprising/exciting about this. All these references to cellular automata (CA) and emergent behavior makes me really excited to see how these local networks are going to start forming a coherent policy.&lt;/li&gt;
  &lt;li&gt;Self-organizing network agents, while being a mouthful, does seem like a natural progression. I’m having trouble explaining why. I wanted to write something about NNs working better on local computation, but they do seem to do global computation pretty well too. I’m not sure this is the right distinction to make (whether NNs are better as local agents or singular global agents). Also there’s so much more research for the latter.&lt;/li&gt;
  &lt;li&gt;Meta-learning policies
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;fast weights&lt;/strong&gt;: adapt weights to input.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;associative weights&lt;/strong&gt;: allow RNN weights to be attracted to recent hidden states.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;hypernetworks&lt;/strong&gt;: one network generates the weights for another.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Hebbian learning&lt;/strong&gt;: See more &lt;a href=&quot;https://arxiv.org/abs/2002.10585&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention
    &lt;ul&gt;
      &lt;li&gt;similar to previous adaptive weight mechanisms in that it modifies weights based on inputs.&lt;/li&gt;
      &lt;li&gt;Authors cite several examples of attention learning inductive biases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Method
    &lt;ul&gt;
      &lt;li&gt;A legit permutation invariant (PI) agent doesn’t need to be trained on permutations to recognize them.&lt;/li&gt;
      &lt;li&gt;PI formulation: need a function $f(x): \mathcal{R}^N \longrightarrow \mathcal{R}^M$ s.t. $ f(x[x]) f(x)$ where $s$ is some permutation of the indices ${1 … n}$&lt;/li&gt;
      &lt;li&gt;Self-attention as PE.
        &lt;ul&gt;
          &lt;li&gt;Simplest self attention: $\sigma(QK^T)V$ where $Q,K \in \mathcal{R^{n\times d_{q}}}$ and $V \in \mathcal{R^{n\times d_{v}}}$. Q,V and K are query, value and key matrices respectively.&lt;/li&gt;
          &lt;li&gt;Normally these (Q,K,V) are functions of the input.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;contribution
        &lt;ul&gt;
          &lt;li&gt;authors propose to add an extra layer (called AttentionNeuron) in front of agent’s policy network $\pi$ that accepts observation $o_t$ and previous action $a_{t-1}$ as inputs.&lt;/li&gt;
          &lt;li&gt;The $i$th neuron only has access to $o_t[i]$.&lt;/li&gt;
          &lt;li&gt;each sensory neuron computes messages $f_k(o_t[i], a_{t-1})$ and $f_v(o_t[i])$.&lt;/li&gt;
          &lt;li&gt;These messages are aggregrated in &lt;em&gt;global latent code&lt;/em&gt; $m_t$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;So I think it’s interesting that based off the figure, they don’t model this AttentionNeuron layer as distinct part of each neuron but rather a layer that recives input from each neuron and aggregates input into $m_t$.
        &lt;ul&gt;
          &lt;li&gt;Computationally, it doesn’t matter if the transformation is affixed to the last layer of each neuron or if it stands alone. But I thought it was illustative of how the Query, Key and Value matrices are formed.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Key matrix depends on $f_k(o_t, a_{t-1})$ and value matrix depends on $f_v(o_t)$&lt;/li&gt;
      &lt;li&gt;They also have projection matrices $W_k, W_v, W_q$ (which is what gives us PE).&lt;/li&gt;
      &lt;li&gt;Experiments and observation space are
        &lt;ul&gt;
          &lt;li&gt;Cartpole $\mathcal R^5$&lt;/li&gt;
          &lt;li&gt;Ant $\mathcal R^28$&lt;/li&gt;
          &lt;li&gt;Car Racing $\mathcal R^{96x96x4}$&lt;/li&gt;
          &lt;li&gt;Atari Pong $\mathcal R^{84x84x4}$
            &lt;ul&gt;
              &lt;li&gt;Car Racing and Atari Pong have dimension $4$ because they are stacked grayscale RGB values.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;They never use more than 196 activation neurons.&lt;/li&gt;
      &lt;li&gt;Also activation neurons do seem to be a function of observation space.&lt;/li&gt;
      &lt;li&gt;Action space ranges from 1 action, to 8 possible actions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Brief discussion of design choices.
    &lt;ul&gt;
      &lt;li&gt;I wonder what the motivation is for making $QW_q$ learnable instead of input dependent.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;They didn’t use a projection matrix for the Value matrix ($V$)&lt;/li&gt;
  &lt;li&gt;The don’t use RNNs for high dimensional input, just feed forward neural network (FFN).&lt;/li&gt;
  &lt;li&gt;Results
    &lt;ul&gt;
      &lt;li&gt;CartPoleSwingUpHarder
        &lt;ul&gt;
          &lt;li&gt;They compare against a two layer FFN trained with CMA-ES&lt;/li&gt;
          &lt;li&gt;The benchmark agent is able to balance the pole faster under normal circumstances because their method requires a few burn in steps.&lt;/li&gt;
          &lt;li&gt;But what is really notable here is that various peterbutations of shuffling the input or concatenating a noisy vector to the input don’t affect performance. No retraining required!
            &lt;ul&gt;
              &lt;li&gt;Might’ve liked to see a comparison against retrained benchmark.&lt;/li&gt;
              &lt;li&gt;Yes I know it’s comparing apples to oranges and it probably wouldn’t make sense to put in a paper … but I’m curious.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;PyBullet Ant
        &lt;ul&gt;
          &lt;li&gt;Uses pre-trained models and behavior cloning for the benchmark.&lt;/li&gt;
          &lt;li&gt;From table 4 the most notable insights I picked up:
            &lt;ul&gt;
              &lt;li&gt;BC works better with larger subsequent layers.&lt;/li&gt;
              &lt;li&gt;ES shuffled works well on their agent as is. Slightly underperforms non-shuffled input version (FFN teacher). Shuffled FFN gets a really bad score.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Atari Pong
        &lt;ul&gt;
          &lt;li&gt;** shuffled atari pong :)&lt;/li&gt;
          &lt;li&gt;I like the idea of maintaining a spatial representation of the 2D grid in $m_t$ because it is more interpretable.&lt;/li&gt;
          &lt;li&gt;I see that this agent can take a subset of the screen and the authors suggest that would be interesting to conduct some kind of ablation experiment on. While I agree, I am also interested in what would happen if you blacked out part of the screen, thus forcing the agent to hallucinate what is happening in those shuffled blocks.&lt;/li&gt;
          &lt;li&gt;The occluded agent rarely won when it couldn’t see certain blocks, but when the blocks are added back it does well.&lt;/li&gt;
          &lt;li&gt;This is the generalization advantage.&lt;/li&gt;
          &lt;li&gt;Figure 6 is a great example of why keeping 2d spatial embeddings is a nice design. The separability of embedding states based on state space is cool to look at.
            &lt;ul&gt;
              &lt;li&gt;Sometimes when I read these papers, I wonder how they coded some things in. This is one I am particularly interested on.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Car racing
        &lt;ul&gt;
          &lt;li&gt;As I guessed in the beginning, attention-based mechanisms are crippled by shuffling inputs.
            &lt;ul&gt;
              &lt;li&gt;This is shown in AttentionAgent failure to do anything but an original task.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;I have no idea how netRand + AttentionAgent are combined and I don’t understand them pretty well.&lt;/li&gt;
          &lt;li&gt;That said, it’s clear that is the only way to compete with the presented method given new unseen and shuffled racing maps.&lt;/li&gt;
          &lt;li&gt;Ah I spoke too soon… it’s explained below the figure.&lt;/li&gt;
          &lt;li&gt;I see they just add AttentionAgent layer at the end but unfortunately, I don’t understand enough about these methods to say anything else.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discussion + Conclusion
    &lt;ul&gt;
      &lt;li&gt;Further uses included dynamic input-output mappings in robots or cross-wired systems.
        &lt;ul&gt;
          &lt;li&gt;This isn’t an entirely broad application – it’s a bit specific to systems that receive shuffled inputs.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;However, I think the most important impact of this paper is that it “provides a lens for understanding a transformer as a self-organizing network” in what I felt was a natural way.
        &lt;ul&gt;
          &lt;li&gt;When I read the title, I imagined the authors would need to make more substantial leaps in connecting transformers with a sensory neuron, but now that I’ve read the paper it feels ok.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;One thing I wonder when reading this paper is: what is the best way to aggregate signals from the individual agents? This paper looked at aggrregation through neural network layers, but I thought the reshape in the 2D spatial embedddings was an interesting twist.
    &lt;ul&gt;
      &lt;li&gt;Generally, this paper adds some interesting ingredients to a mixture of local -&amp;gt; global computation using distributed agents.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang</summary></entry><entry><title type="html">Review 8: The Future of Artificial Intelligence is Self-Organizing and Self-Assembling</title><link href="http://localhost:4000/reviews/review8/" rel="alternate" type="text/html" title="Review 8: The Future of Artificial Intelligence is Self-Organizing and Self-Assembling" /><published>2022-01-02T13:16:13-08:00</published><updated>2022-01-02T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review8</id><content type="html" xml:base="http://localhost:4000/reviews/review8/">&lt;p&gt;&lt;a href=&quot;https://sebastianrisi.com/self_assembling_ai/9&quot;&gt;The Future of Artificial Intelligence is Self-Organizing and Self-Assembling&lt;/a&gt; by &lt;a href=&quot;https://sebastianrisi.com&quot;&gt;Prof. Sebastian Risi&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Citation: Risi, Sebastian. “The Future of Artificial Intelligence is Self-Organizing and Self-Assembling”. sebastianrisi. com (2021): n. pag. Web.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Today I am reading an interesting blog post by Professor Risi, at the &lt;a href=&quot;https://www.itu.dk/&quot;&gt;IT University of Copenhagen&lt;/a&gt;, about self organizing systems and how they can potentially solve some of the problems in developing generalizable AI.&lt;/li&gt;
  &lt;li&gt;The blog post starts by detailing the incredible scale at which systems self-organize. I can’t help but recall an interesting tweet by &lt;a href=&quot;https://twitter.com/plinz?lang=en&quot;&gt;Joscha Bach&lt;/a&gt; that says something to the effect of “it’s computation all the way down”. My interpretation of this saying is that at every level of these complex systems, there are smaller and smaller constituents responsible for performing preceding simpler and simpler computations. In this way, a single ant can be responsible for finding a piece of food and then remembering the path to bring the food back home. However, the delegation of tasks to these ants allows a complex colony of ants to survive and ants are notoriously good at surviving. The problem with computation all the way down, is that the computation solves a simpler problem, but that doesn’t mean the computation itself is simple. This is why I think Prof. Risi brings up a good point in the first paragraph – if we want the benefit of robustness – we’ll need to find efficient training methods.&lt;/li&gt;
  &lt;li&gt;The difference being drawn out between self-assembly and emergence is fascinating. Here’s why I think so. As an engineer (software engineer for the most part), I’ve started to realize how disastrous “over-engineering” is. The more code I have to write in order to extend some system for a simple problem is often a design flaw I made in the past. Amazingly, emergent systems like ants or swarms of insects don’t have this problem since their genetic code sets them up so well to carry out their &lt;em&gt;local&lt;/em&gt; and &lt;em&gt;global&lt;/em&gt; purposes. Historically, we are pretty damn good engineers, but when it comes to endowing our designs with this kind of adaptability, we often come up short. Our work is constantly being replaced, rebuilt, refactored, repurposed.
    &lt;ul&gt;
      &lt;li&gt;These words are inherent to human creations. Nature never refactors. It may replace or repurpose at times, but in the slowest timescale imaginable. Humans are always rushing to get these things done. To get a new version released, to improve state of the art, to hastily revitalize old infrastructure.&lt;/li&gt;
      &lt;li&gt;One caveat to this point is that humans don’t build systems capable of robust adaptation: art. Great artists build things that seem to be constant sources of beauty (or functionality depending on what you expect out of art). For example, the Mona Lisa has been captivating for hundreds of years. Tchaikovsky’s music doesn’t need any refactoring or &lt;em&gt;remixing&lt;/em&gt;. On the other hand, it’s hard to argue that these things have some emergent behavior.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;I wonder how true it is that neural nets can’t generalize very well. I agree with Prof. Risi’s claim, but I think GPT-3 is an interesting counter-example. Sure it’s a giant net, but it can handily craft human-level responses to questions outside of its training data.&lt;/li&gt;
  &lt;li&gt;I will continue reading but these references are hard to ignore!
    &lt;blockquote&gt;
      &lt;p&gt;Vichniac, 1984; Wolfram, 1984; Langton, 1986 qtd. in Risi 2021&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;Cellular Automata can aid in the understanding of biological pattern formations, modeling of lattice-based physical systems in percolation and nucleation, and synthesis of artificial life.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://xanderladd.github.io/research/benchmarking&quot;&gt;My research&lt;/a&gt; involves evolutionary alorithms. So Neural Cellular Automata (NCAs) seem really interesting to me.
    &lt;ul&gt;
      &lt;li&gt;However, the fact that reassembling the Nordic flag causes non-optimal solutions gives a good idea that conquering challenging optimization landscapes can be a real tough problem.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=7fFO4cMBx_9&quot;&gt;Variational Neural Cellular Automata&lt;/a&gt; reminds me I need to learn how VAEs work a bit better!&lt;/li&gt;
  &lt;li&gt;Their &lt;a href=&quot;https://sebastianrisi.com/wp-content/uploads/minecraft_short.mp4&quot;&gt;3D Minecraft NCA tree house morphogenesis video&lt;/a&gt; is really cool. I am amazing by the detail of the reconstruction of their algo. Though I’ve never played minecraft.&lt;/li&gt;
  &lt;li&gt;The really interesting part behind their demonstration of locomotive structures is that the interactions are developed at a local level. This kind of interaction structure is the same kind the brain uses.
    &lt;ul&gt;
      &lt;li&gt;For the most part.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The above is confirmed by Prof. Risi here:
    &lt;blockquote&gt;
      &lt;p&gt;Risi 2021&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;For example, instead of optimizing the weight parameters of neural networks directly, only meta-learning synapse-specific Hebbian learning rules allows a network to continuously self-organize its weights during the lifetime of the agent (Najarro &amp;amp; Risi, 2020). We found that starting from completely random weights, evolved Hebbian rules enable an agent to navigate a dynamic 2D-pixel environment; likewise, the approach also allows a simulated 3D quadruped to learn how to walk while adapting to some morphological damage not seen during training and in the absence of any explicit reward or error signal in less than 100 timesteps.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;Also, the “not seen during training example” &lt;a href=&quot;https://sebastianrisi.com/wp-content/uploads/ant_hebbian2.mp4&quot;&gt;here&lt;/a&gt; is pretty funny as it just flops around.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The author describes several methods of parameter sharing:
    &lt;ol&gt;
      &lt;li&gt;Iteratively merging learning rules
        &lt;ul&gt;
          &lt;li&gt;based on the “genomic bottleneck” (&lt;a href=&quot;https://www.nature.com/articles/s41467-019-11786-6&quot;&gt;Zador 2019&lt;/a&gt;), they drastically reduce parameters by combining rules. (what is k-means being used on?)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Learning learning rules through self-organization
        &lt;ul&gt;
          &lt;li&gt;OK, so this reference to Krisch and Schmidhuber is a must-read. &lt;a href=&quot;https://arxiv.org/abs/2012.14905&quot;&gt;https://arxiv.org/abs/2012.14905&lt;/a&gt;. Basically, small recurrent networks somehow self-organize and learn a global algorithm. Really want to find out more here.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Learning to be robust to unseen and permutated sensory inputs
        &lt;ul&gt;
          &lt;li&gt;Idea here is to add a global controller or attention mechanism to moderate local interactions. This can provide some robustness against variation.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Now time to discuss training. Prof. Risi says this is hard because:
    &lt;ol&gt;
      &lt;li&gt;There is no entity that is essentially fully in charge of the self-organizing system, no engineer at the helm.&lt;/li&gt;
      &lt;li&gt;These systems are chaotic, it’s hard to analytically guide them to stable states. The solutions are non-deterministic and often degenerate to local minima.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;So what can we do?
    &lt;ul&gt;
      &lt;li&gt;Quality Diversity (Pugh et al. 2016)[https://www.frontiersin.org/articles/10.3389/frobt.2016.00040/full] aims to generate a diverse set of solutions instead of only one satisfying solution.&lt;/li&gt;
      &lt;li&gt;Deep learning + Compositional Pattern Producing Networks (CPPNs; Stanley 2007) &lt;a href=&quot;https://arxiv.org/abs/1908.06663&quot;&gt;Reinke et al. 2020,&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;I got lost here. I don’t know what CPNNs do and unfortunately, I already added enough to my reading list for today.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Overall I thought this blog post did a great job of explaining several distinct pros and cons to distributed learning systems. It seems like these methods are becoming an &lt;em&gt;emerging trend&lt;/em&gt; (nod nod wink wink) in machine learning. I think even Geoff Hinton’s work on GLOM (&lt;a href=&quot;https://arxiv.org/pdf/2102.12627.pdf&quot;&gt;Hinton 2020&lt;/a&gt;) has some resemblance to distributed systems. Ultimately, neural networks have done an excellent job of stacking computational units together, but the opportunity for robustness in diverse and locally structured systems is exciting. It’s awesome to see so many examples tying deep learning networks into this concept of self-organizing systems. Outside of evolutionary algorithms, I don’t know much about this stuff but I’ll be following along!&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">The Future of Artificial Intelligence is Self-Organizing and Self-Assembling by Prof. Sebastian Risi</summary></entry><entry><title type="html">Review 7: Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors</title><link href="http://localhost:4000/reviews/review7/" rel="alternate" type="text/html" title="Review 7: Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors" /><published>2021-12-26T13:16:13-08:00</published><updated>2021-12-26T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review7</id><content type="html" xml:base="http://localhost:4000/reviews/review7/">&lt;p&gt;&lt;a href=&quot;https://elifesciences.org/articles/66809&quot;&gt;Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors&lt;/a&gt; by &lt;a href=&quot;http://neuroscience.bs.jhmi.edu/huganir/Member/AustinGraves.html&quot;&gt;Dr. Austin Graves&lt;/a&gt; &lt;a href=&quot;https://www.hopkinsmedicine.org/profiles/details/richard-huganir&quot;&gt;Dr. Richard Huganir&lt;/a&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Citation: Graves, Austin R., et al. “Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors.” Elife 10 (2021): e66809.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;First, I like how this article is structured. It has introduction, then just goes right into results. This is well tailored to the reader because unless you are in some very theoretical field and the method is &lt;em&gt;somehow&lt;/em&gt; more important than the result, we want to read the results.&lt;/li&gt;
  &lt;li&gt;By the way, I had no idea what a SEP tag is or even what knockin means! I also didn’t know what GluA1 does.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;knockin&lt;/strong&gt;: mouse genetic lines that have an altered gene sequence.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;SEP&lt;/strong&gt;:  super ecliptic pHluorin (SEP), flouresces at neutral Ph only meaning it will be expressed on the cell membrane, but not inside the cell. fluorescent tag for AMPAR (AMPA receptors)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;GluA1&lt;/strong&gt;: Glutamate receptor protein that forms AMPAR&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The begininning the results section focuses on how their SEP-GluA1 knockin mouse line is functionally the same as the wildtype (WT) mouse line.
    &lt;ul&gt;
      &lt;li&gt;The authors do this thoroughly using several experiments.
        &lt;ol&gt;
          &lt;li&gt;Northern and Western blot flourescence protocols to determine mRNA protein expression of GluA1, GluA2, and GluA3 in knockin line and WT line. This is done using slice of mouse hippocampal tissue. (Fig 1)
            &lt;ul&gt;
              &lt;li&gt;They noticed that in their knockin mice, GluA1 postsynaptic density is lowered due to the knockin line mutation of the mRNA. Then knockin line mice have higher GluA3 to compensate.&lt;/li&gt;
              &lt;li&gt;However, this doesn’t seem to have any functional effects on synpatic plasticity – as the authors go on to show.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Single voltage clamp records of hippocampal slices (Fig 2), which show that the synaptic behavior of SEP-GluA1 knockin mice is unaffected by the tag.
            &lt;ul&gt;
              &lt;li&gt;They show this by showing that the EPSC (excitatory post synpatic currents) are unaffected by SEP tagging.&lt;/li&gt;
              &lt;li&gt;They show this under LTP induction too in the next step.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Compounds like TTX and BIC have homogenous effects on relative surface AMPA recepots of both WT and SEP-GluA1 knockin mice.&lt;/li&gt;
          &lt;li&gt;Behavioral experiments, where SEP-GluA1 mice had the same behavior on locomotor task, anxiety task, spatial novelty prefence task.
            &lt;ul&gt;
              &lt;li&gt;I know the difference is not significant but in the anxiety task it does look like SEP-GluA1 mice &lt;em&gt;do&lt;/em&gt; spend more time in the open arms. 
        - Overall, this battery of evidence is convincing that we can consider SEP-GluA1 mice as representative of WT mice.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Now to discuss the good part. What happens to these synapses at large scale.&lt;/li&gt;
  &lt;li&gt;First, this is good to know:
    &lt;blockquote&gt;
      &lt;p&gt;Graves et al., 2021&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;We found a significant correlation between SEP-GluA1 fluorescence intensity and uEPSC amplitude (Figure 5c), indicating that SEP fluorescent intensity can be used as a proxy for synaptic strength.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;Glutamate uncaging led to increased uEPSC amplitude and SEP fluorescence in stimulated spines.
    &lt;ul&gt;
      &lt;li&gt;glutamate uncaging is light induced photstimulation of neurons.&lt;/li&gt;
      &lt;li&gt;authors show that as expected, this induced stimulation of neurons likely results in more AMPAR receptors on the cell membrane as a marker of increased synaptic strength.&lt;/li&gt;
      &lt;li&gt;Shown clearly in Figure 5D-F. These are important figures. They show that LTP in high frequency pairings is marked by an incresed SEP flourescence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Even &lt;em&gt;in vitro&lt;/em&gt; there’s a strong correlation between SEP fluorescence and EPSC amplitude.&lt;/li&gt;
  &lt;li&gt;Paper presents an unsupervised algorithm for synapse detection.&lt;/li&gt;
  &lt;li&gt;Sensory stimulation task
    &lt;ul&gt;
      &lt;li&gt;The detection algorithm recognizes &lt;strong&gt;hundreds of thousands of synapses&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;The experimenter stimulates one mouse whisker but not the other.&lt;/li&gt;
      &lt;li&gt;The C2 barrel corresponding to stimulated whisker displays a higher SEP-GluA1 intensity from the whisker stimulation induced LTP.&lt;/li&gt;
      &lt;li&gt;The figure showing this (Fig 8c.) shows this result clearly and it is statisically significant.&lt;/li&gt;
      &lt;li&gt;Not only that, we get the time scale this happened on as well. (3hrs)
        &lt;ul&gt;
          &lt;li&gt;well likely the effect just observed in post stim imaging actually.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This sensory stimulation task is awesome for a few reasons.
    &lt;ol&gt;
      &lt;li&gt;They have a ton of data since they detect so many synapses. This means when something is significant it has an N &amp;gt; 100,00, which is very convincing.&lt;/li&gt;
      &lt;li&gt;I &lt;em&gt;think&lt;/em&gt; this is the first experiment demonstrating one of the actual mechaisms of LTP &lt;em&gt;in vivo&lt;/em&gt; and that’s pretty cool.&lt;/li&gt;
      &lt;li&gt;This is just showing a sensory stimulation task. I wonder what else this methodology can show!&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors by Dr. Austin Graves Dr. Richard Huganir.</summary></entry><entry><title type="html">Review 6: Spiking Neural Nets</title><link href="http://localhost:4000/reviews/review6/" rel="alternate" type="text/html" title="Review 6: Spiking Neural Nets" /><published>2021-12-19T13:16:13-08:00</published><updated>2021-12-19T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review6</id><content type="html" xml:base="http://localhost:4000/reviews/review6/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://simons.berkeley.edu/news/spiking-neural-networks&quot;&gt;Spiking Neural Networks&lt;/a&gt; by &lt;a href=&quot;http://anilananthaswamy.com/&quot;&gt;Anil Ananthaswamy&lt;/a&gt;.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Citation: &lt;a href=&quot;http://anilananthaswamy.com/&quot;&gt;Anil Ananthaswamy&lt;/a&gt;, “Spiking Neural Networks” Simons Institute, Dec 13, 2021&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;I believe it is immensly understated how critical understanding the dynamics of spiking neural networks (SNN) is to advancing &lt;em&gt;energy efficient&lt;/em&gt; machine intellgience. This quote from this article sums it up perfectly:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;computational neuroscientist Friedemann Zenke qtd. in Ananthaswamy 2021&lt;/p&gt;
      &lt;blockquote&gt;
        &lt;p&gt;My main motivation to think about spiking neural networks is because it’s the language the brain speaks.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Because SNNs are not researched as much as other types of artificial neural nets (ANNs), I really enjoyed the historical perspective Anil presents here. This writing shows how deep ANNs like AlexNet put deep learning on a central stage and how SNNs have stayed backstage due to a variety of tractability issues. A review of such issues is as such:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Backprop and non-differentiablity of SNNs:&lt;/strong&gt; the first challenge is that spiking neurons are often using non-continous and discrete functions like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Heaviside_step_function&quot;&gt;&lt;em&gt;step function&lt;/em&gt;&lt;/a&gt; which prevent researchers from perforiming backprop. See &lt;a href=&quot;https://homepages.cwi.nl/~sbohte/research.html&quot;&gt;Dr. Sander Bohte’s&lt;/a&gt; approach to this problem.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Memory storage and backprop through time:&lt;/strong&gt; assuming we can compute gradients (or approximate ones), we now need to deal with the fact the human neural nets show spiking latency at several different temporal scales. This is a problem becuase encoding neuron states at the scale of miliseconds for an hour of activity is incredibly memory ineffcient. Especially considering neurons exhibit &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3154835/&quot;&gt;“lifetime sparseness”&lt;/a&gt;(Quiroga and Kreiman 2016), which means that in the brain, they don’t really fire that often. As a result, we end up encoding a lot of useless information about a neuron’s membrane potential.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Single cell activation functions:&lt;/strong&gt; This issue isn’t directly cited here, but I’d like to use it to group a few issues together. The issue that I think it’s most related to in the text is: &lt;em&gt;Homogeneous vs. heterogenous cell types&lt;/em&gt;. This is the idea that having neurons that vary in comparison to eachother and have different firing rules and thresholds allows the information to encode more information and actually results in improved prediction in temporally structured tasks, such as &lt;a href=&quot;https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/&quot;&gt;SNN Audio MNIST from Zenke Lab&lt;/a&gt;. I’d like to add some more considerations here beyond homogeneity and heterogeneity. There are two other factors not mentioned here that result in heterogeneity. First, I think synapse types are important. Many researchers work on excitation and inhibition (E/I) balance in the brain has led to better understanding how attractor states might occur in the brain (see my &lt;a href=&quot;https://xanderladd.github.io/reviews/review3/&quot;&gt;previous review&lt;/a&gt;). Second, I don’t have many references for this on-hand, but any experienced EEG practicioner will tell you how much background activity and local field potentials impact single neuron firing rates. I think these ideas have been experimented with by SNN researchers, but aren’t quite as impactful as the cited results by &lt;a href=&quot;https://neural-reckoning.org/dan_goodman.html&quot;&gt;Goodman lab&lt;/a&gt; on heterogenous neuron types resulting in a 15-20% increase in accuracy on audio MNIST.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Anil also presents an array of ingenious research solutions to these problems such as EventProp (&lt;a href=&quot;https://scholar.google.com/citations?user=FzRMI38AAAAJ&amp;amp;hl=en&quot;&gt;Dr. Pehle 2021&lt;/a&gt;) and Surrogate Gradients (Neftci, Mostafa, and Zenke). These are covered pretty well in the original article. However, looking at this from a broader perspective, these approaches don’t seem like they’ve lead to anything that performs so well on MNIST or any other SOTA ML result that researchers enamored with transformers are going to jump ship anytime soon. Looking at this article within the broader context of consolidation with ML/AI, written about by &lt;a href=&quot;https://twitter.com/karpathy/status/1468370605229547522&quot;&gt;Andrej Karpathy&lt;/a&gt;, it seems most ML and AI researchers are focused on ANN based deep learning and are unlikely to migrate over just because of surrogate gradients. I don’t think this is a bad thing for either focus – in fact it’s a good thing &lt;em&gt;for both&lt;/em&gt;. No matter how open minded we frame ourselves, most researchers are biased towards their own research direction. We have to keep reinforcing a narrative that studying &lt;em&gt;X&lt;/em&gt; is more interesting than stuyding &lt;em&gt;Y&lt;/em&gt; for reason &lt;em&gt;J&lt;/em&gt;,&lt;em&gt;K&lt;/em&gt; and &lt;em&gt;L&lt;/em&gt;. This means that SNN researchers are either going to have to show that SNNs can be &lt;strong&gt;significantly&lt;/strong&gt; more energy efficent, or better yet &lt;strong&gt;convincingly&lt;/strong&gt; more accurate. Likely, the case for SNNs is as a worthwile tradeoff between accuracy and energy efficiency.&lt;/p&gt;

    &lt;p&gt;As a enthusiast of biologically inspired networks, I find myself rooting for the future promise of SNNs. However, at the end of the day, for a field of machine intelligence that is extremely based on comparison and benchmarks, the world is not so simple. Mean absolute error (MAE) is important, but not all encompassing. &lt;a href=&quot;https://profiles.uts.edu.au/Tara.Hamilton&quot;&gt;Dr. Tara Hamilton’s&lt;/a&gt; presentation at &lt;a href=&quot;http://snufa.net/2021/&quot;&gt;SNUFA 2021&lt;/a&gt; really helped me realize that there are plently of insteresting use cases for SNNs, for example cochlea implants or heart rate monitoring. However, when it comes to self-driving cars, convolutional neural networks (CNNs) remain an obvious choice. This dynamic is actually a really amazing thing. However, as a researcher, I have to go back to the original quote, that SNNs are the language of the brain. For someone who is interested in reverse engineering the brain, I think it’s hard not consider SNNs an engaging model problem. My own experience with SNNs is very beginner, but it’s posts like this one by Anil that set the stage for an exciting future in the field.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Spiking Neural Networks by Anil Ananthaswamy.</summary></entry><entry><title type="html">Review 5</title><link href="http://localhost:4000/reviews/review5/" rel="alternate" type="text/html" title="Review 5" /><published>2021-12-12T13:16:13-08:00</published><updated>2021-12-12T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review5</id><content type="html" xml:base="http://localhost:4000/reviews/review5/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://bair.berkeley.edu/blog/2021/10/22/mural/&quot;&gt;Making RL Tractable by Learning More Informative Reward Functions: Example-Based Control, Meta-Learning, and Normalized Maximum Likelihood&lt;/a&gt;.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Citation: &lt;a href=&quot;https://abhishekunique.github.io/&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/kevintli/&quot;&gt;Kevin Li&lt;/a&gt;, and &lt;a href=&quot;http://people.eecs.berkeley.edu/~svlevine/&quot;&gt;Sergey Levine&lt;/a&gt; “Making RL Tractable by Learning More Informative Reward Functions: Example-Based Control, Meta-Learning, and Normalized Maximum Likelihood” BAIR Blog, Oct 22, 2021&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;A limiting factor in RL is having reward functions that accurately represent the intended goal of the actions. There’s a gap between what the reward function specifies and what we actually want it to specify.&lt;/li&gt;
      &lt;li&gt;Also, there’s a trade off with having reward functions that are specific to an environment and having reward function that can generalize well.&lt;/li&gt;
      &lt;li&gt;The authors introduce a basic reward function as classifying an action as successful or unsuccessful.
        &lt;ul&gt;
          &lt;li&gt;works well in limited settings where a lot of exploration is not required.&lt;/li&gt;
          &lt;li&gt;downside is that neural network based classifiers, even with some kind of regularization can converge upon sub optimal policies.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;They describe this downside as “overconfidence”.&lt;/li&gt;
      &lt;li&gt;Conditional Normalized maximum likelihood (CNML) to add uncertainty in the model.
        &lt;ul&gt;
          &lt;li&gt;Given query point $x$ and data $\mathcal{D}[x_0, y_0, … , x_N, y_N]$ CNML solves \begin{equation}
  \theta_i = argmax_{\theta}\mathbb{E}_{D \cup (x_q, \mathcal{C}_i)} \mathcal{L}(f(x_q), y)
  \end{equation}
  Where there are $\mathcal{C}_i \in [ \mathcal{C}_0, \mathcal{C}_k ]$, so $k$ possible classes outcome. $x_q$ is the current point. This policy makes a prediction for each class (because do the above calculation for $i \in k$), so they suggest normalizing.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;\(p_{CNML}(\mathcal{C}_i \mid x) = \frac { f_{\theta_i}(x)} {\sum\limits_{j=1}^{k}f_{\theta_j}(x)}\)&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;This reward policy is better equipped to handle exploring in uncertain situations.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The authors then discuss generalization to continuous valued states.&lt;/li&gt;
      &lt;li&gt;The authors discuss tractability of solving $k$ NMLE equations for every potential action. This is slow.&lt;/li&gt;
      &lt;li&gt;Then they introduce the meta learning algorithm.
        &lt;ul&gt;
          &lt;li&gt;I kind of lost the thread here.&lt;/li&gt;
          &lt;li&gt;train another model to predict outcome $2N$ actions corresponding to postitive and negative likelihood.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Final thoughts.
        &lt;ul&gt;
          &lt;li&gt;The core algorithm is really simple which is awesome but I am not entirely sure about the meta learning part. Here’s my interpreation:
            &lt;ul&gt;
              &lt;li&gt;You sample $n$ points from the replay buffer and try train another model to predict what the NML will be, given that the point is classified as helpful and unhelpful.&lt;/li&gt;
              &lt;li&gt;For the maze problem, this should start bulding a NML surrogate model that has a good spatial understanding of how different helpful and unhelpful point layouts shape the map.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Making RL Tractable by Learning More Informative Reward Functions: Example-Based Control, Meta-Learning, and Normalized Maximum Likelihood.</summary></entry><entry><title type="html">Not Even Wrong</title><link href="http://localhost:4000/posts/Not_Even_Wrong/" rel="alternate" type="text/html" title="Not Even Wrong" /><published>2021-12-06T13:16:13-08:00</published><updated>2021-12-06T13:16:13-08:00</updated><id>http://localhost:4000/posts/Not_Even_Wrong</id><content type="html" xml:base="http://localhost:4000/posts/Not_Even_Wrong/">&lt;p&gt;Wow, I just found out what it means to be “Not even wrong”. The phase, coined by &lt;a href=&quot;https://en.wikipedia.org/wiki/Wolfgang_Pauli&quot;&gt;Wolfgang Pauli&lt;/a&gt; has just given me a huge slice of humble pie. Basically not even wrong is used to describe a totally failed conjecture. If something is meaningless, it’s definitely not right, but it’s not wrong either since you have to be coherent and pointed to be wrong. This has changed the way I think about wrong and right in science. Sometimes, as all scientists do, I use big words when I’m not exactly sure what I’m talking about to make it seem like I do. And the truth is, there’s no cookie and no medal for being hard to understand. From now on, if I’m unsure but I stil want to conjecture, I am going to do it with as basic of an explnation as possible. This way, if I’m misguided I’ll get caught fast instead of getting by on jibberish. This is the route to actually being right about a lot of things one day. Well, I hope that day comes, for now I’d rather be wrong than not even wrong.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doing_it_wrong.jpeg&quot; alt=&quot;you're doing it wrong!&quot; width=&quot;500&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;</content><author><name>xander</name></author><category term="posts" /><summary type="html">Wow, I just found out what it means to be “Not even wrong”. The phase, coined by Wolfgang Pauli has just given me a huge slice of humble pie. Basically not even wrong is used to describe a totally failed conjecture. If something is meaningless, it’s definitely not right, but it’s not wrong either since you have to be coherent and pointed to be wrong. This has changed the way I think about wrong and right in science. Sometimes, as all scientists do, I use big words when I’m not exactly sure what I’m talking about to make it seem like I do. And the truth is, there’s no cookie and no medal for being hard to understand. From now on, if I’m unsure but I stil want to conjecture, I am going to do it with as basic of an explnation as possible. This way, if I’m misguided I’ll get caught fast instead of getting by on jibberish. This is the route to actually being right about a lot of things one day. Well, I hope that day comes, for now I’d rather be wrong than not even wrong.</summary></entry><entry><title type="html">Review 3</title><link href="http://localhost:4000/reviews/review3/" rel="alternate" type="text/html" title="Review 3" /><published>2021-11-18T13:16:13-08:00</published><updated>2021-11-18T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review3</id><content type="html" xml:base="http://localhost:4000/reviews/review3/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0959438821000593&quot;&gt;From synapse to network: models of information storage and retrieval in neural circuits
&lt;/a&gt; by &lt;a href=&quot;https://aljadeff.ucsd.edu/&quot;&gt;Johnatan (Yonatan) Aljadeff&lt;/a&gt; et al.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Citation: Aljadeff, Johnatan, et al. “From synapse to network: models of information storage and retrieval in neural circuits.” Current opinion in neurobiology 70 (2021): 24-33.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;Intro
        &lt;ul&gt;
          &lt;li&gt;Stimuli dependent patterns in neural circuity lead to research in mechanisms for synaptic updates and and how such modifications change network dynamics.&lt;/li&gt;
          &lt;li&gt;Introduces two theoretical approachs to modeling dynamics of delay periods in updates – biophysical connecitivity matrix and supervised learning approaches.&lt;/li&gt;
          &lt;li&gt;This paper is gonna look at the former.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Synaptic Plasticity rules
        &lt;ul&gt;
          &lt;li&gt;Some STDP curves seem useful &lt;em&gt;in vitro&lt;/em&gt; butthe calcium dynamics &lt;em&gt;in vivo&lt;/em&gt; seem to alter these rules.&lt;/li&gt;
          &lt;li&gt;In inferring directly from &lt;em&gt;in vivo&lt;/em&gt; theres a limitation in measuring both pre and post synaptic acitivty.
            &lt;ul&gt;
              &lt;li&gt;I’m curious how this limitaiton works. Is it a resolution thing, or is the issue that it’s hard to dissociate between pre and post synaptic updates?&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Lim et al. study seems to be leading as far as infering in vitro pre synaptic rules.&lt;/li&gt;
          &lt;li&gt;So far ML models are being developed but only shown success on synthetic data.&lt;/li&gt;
          &lt;li&gt;plasticity can be induced by pairing presynaptic spikes with postsynaptic plateu potentials over longer timecales.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Network dynamics
        &lt;ul&gt;
          &lt;li&gt;What type of network dynamics emerger from networks with these learning rules? Two types of scenarios, one where learning and retireval happen in seperate phases and another where the happen concurrently.&lt;/li&gt;
          &lt;li&gt;Netowrks with learning seperate from retrieval
            &lt;ul&gt;
              &lt;li&gt;These networks demonstrate (converge?) to attractor states&lt;/li&gt;
              &lt;li&gt;A lot of recurrent activity represents higher chatoic acitivity and less like to converge to solid representations but this is similar to the affect of delayed reponses tasks, where dynamics are confined to a stable subspace but are chaotic within that constraint.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Q about figure 2: what do the colorful triangles mean here? I see the caption and that they are activited based on some corresponding output. But I wonder in the larger context here. Are these considered attractor states by definition? Or are there some caveats. Is there are strict defnition that can define these sets, or they just seem to be active corresponding to some external input, given some plasticity rule?&lt;/li&gt;
          &lt;li&gt;AH, question is answered here – seems theres a paper on this and that they can be dubbed “hebbian assemblies” – cool.
            &lt;ul&gt;
              &lt;li&gt;Gotta love when all I have to do to have a question answered is keep reading. :blush:&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Networks with dynamic connectivity
        &lt;ul&gt;
          &lt;li&gt;needs additional stabilization betsides hebbian synaptic plasticity.&lt;/li&gt;
          &lt;li&gt;Here’s where E/I balance and recurrent connections come in.&lt;/li&gt;
          &lt;li&gt;Figure 3 panel I is really cool, there really seems to be some kind of normal pattern behind the overlap when different ensembles are activated. (I think I am understandign this figure correctly)&lt;/li&gt;
          &lt;li&gt;One thing I wonder about the figure is what happens when several stimuli are presented in a sequence. What happens to fraction of overlap in network state then? How often do these small activated ensembles overlap?&lt;/li&gt;
          &lt;li&gt;Also I liked the column showing the subspaces.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Storage Capcacity
        &lt;ul&gt;
          &lt;li&gt;Ah, seems to go nicely with the previous questiona about encoding multiple stimuli in the network.&lt;/li&gt;
          &lt;li&gt;Do biological netowrks reach optimal storage capcity as given by information encoding optimality gurantees on state space of connectivity matrix?&lt;/li&gt;
          &lt;li&gt;Answer to this question depends on the constraints of state space for optimality gurantee – general unsuprivied = open question, hebbian unsupervied = networks seem to use this capcity.&lt;/li&gt;
          &lt;li&gt;Likely we are more in the second case given other factors that encode info (like E/I ratio)&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt; Garder bound seems important for this, I should learn about it.&lt;/li&gt;
          &lt;li&gt;Wow, seems that optimizing for information storage can recreate non trivial aspects of synaptic connectivity. Several studies cited here, could be interesting further reading since these properties seem new to me (except E/I balance).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Discussion
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt; Ref 33 seems really interesting as challenge to persistent activity hypothesis.&lt;/li&gt;
          &lt;li&gt;Temporal statistics here seem to be a real sticking point, hebbian networks can be persistent or sequential (what is squential btw) based on the statistics of the input during learning.
            &lt;ul&gt;
              &lt;li&gt;This isn’t what I thought the autor meant by temporal statisics. I thought they would be a post hoc test, not a pre (or input related) thing. Here is a gap in my knowledge. &lt;strong&gt;Note&lt;/strong&gt; ref 49 might be an intersting follow up.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Approach outlined here: biophysical synaptic updates can be contasted with supervised learning apporaches for different pros and cons.
            &lt;ul&gt;
              &lt;li&gt;Con for supervised learning is that some plasticity rules do satisfy locality constraints and are not biologicallly possible.&lt;/li&gt;
              &lt;li&gt;integration of unsupervised and reward based into this approach will be good future work.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Can progress in experimental (reponse tasks) or computational (advancing biologically plausbile update rules in models) directions.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">From synapse to network: models of information storage and retrieval in neural circuits by Johnatan (Yonatan) Aljadeff et al.</summary></entry><entry><title type="html">Review 4</title><link href="http://localhost:4000/reviews/review4/" rel="alternate" type="text/html" title="Review 4" /><published>2021-11-18T13:16:13-08:00</published><updated>2021-11-18T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review4</id><content type="html" xml:base="http://localhost:4000/reviews/review4/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.talyarkoni.org/blog/2018/08/18/if-we-already-understood-the-brain-would-we-even-know-it/&quot;&gt;If we already understood the brain, would we even know it?&lt;/a&gt; by &lt;a href=&quot;https://talyarkoni.org/&quot;&gt;Tal Yarkoni&lt;/a&gt;.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Citation: Yarkoni, Tal. “If We Already Understood the Brain, Would We Even Know It?” Tal Yarkoni Blog, 29 Aug. 2019, https://www.talyarkoni.org/blog/2018/08/18/if-we-already-understood-the-brain-would-we-even-know-it/.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;First, I like this article already. If I had a dollar for everytime I hear a neuroscientist bemoan “we know nothing about the brain”…&lt;/li&gt;
      &lt;li&gt;The assesment that we know ~ .1 % of the things there are to know about neuroscience is an interesting one for sure. Also that in 200 years or so we can shut down neuroscience.
        &lt;ul&gt;
          &lt;li&gt;This is a reason I am so happy I reviewed a blog instead of a paper today. Finally, we can speculate for arugments sake. Makes reading way more interesting then having to use axioms, posulates and citations for every claim made.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The dissonance in the relationship between our personal understanding of the field and the cumulative knowledge in the field is really a bit concerning. I think this is pointing out that the emperor has no clothes.
        &lt;ul&gt;
          &lt;li&gt;In order to build the skills to be an excellent experimental scientist one must spend a lot of time working hands on in a lab.&lt;/li&gt;
          &lt;li&gt;On the other hand, to understand the field they must spend so much time reading.&lt;/li&gt;
          &lt;li&gt;There’s a tension here that like Tal says, causes us to ask the same question in different ways quite often.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Oooh, it is a tough question to wonder “why does such and such network to activate?”
        &lt;ul&gt;
          &lt;li&gt;We can sometimes answer things like &lt;em&gt;when&lt;/em&gt;, &lt;em&gt;where&lt;/em&gt;, &lt;em&gt;what&lt;/em&gt; etc. but why is difficult.&lt;/li&gt;
          &lt;li&gt;This is on the DMN example.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;As I understand the general intelligence part of this blog the message is this: g_f, or general intelligence, is an effect with a slew of correlated causes (mechanisms involving attention, abstraciton etc.) and to try and pick out &lt;em&gt;the&lt;/em&gt; cause of intellgence is a losing propostion. general intelligence emerges among many influencing factors.&lt;/li&gt;
      &lt;li&gt;This seems like a microcosm of the problem. We are looking for short answers to long quetions. If we want to “fully understand” how a mechanism or circuit works, it’ll invovle synthesizing complicated information at scales from mico to macro.
        &lt;ul&gt;
          &lt;li&gt;This reminds me of integrative biology class tests where I had to detail 5 or 10 steps in a response on autonomous nervous system. The nice part about this is we often looked at one level of complexity. We’d never be asked about the morphological changes to the neurons in the circuit becaue that’s just way too much information to have synthesized – it’s a big jump.&lt;/li&gt;
          &lt;li&gt;In this sense, the full answers to how circuitry in the brain work aren’t obligated to be nice and digestable – but our explanations are.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;I like that Dan is always asking for a &lt;em&gt;core principle&lt;/em&gt; and D’Ann is always just denying the simplicity of one &lt;em&gt;core principle&lt;/em&gt; existing.&lt;/li&gt;
      &lt;li&gt;And we leave off the article basically on the same note.
        &lt;ul&gt;
          &lt;li&gt;Sure we might be able to learn more and more detailed and expressive computational models and these models may even be able to account for all the variance we can hope for … but at this point we’d probably still be asking “what’s the underlying principle?”.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;I think this dilemma relates back to the Searle’s chinese room argument (Searle 2009), it’s like sure you can make make a latent space model that’s very expressive of the mechanism but this is all input/output … you haven’t truly learned the language.&lt;/li&gt;
      &lt;li&gt;I like to think that Tal would agree with me here: The computation &lt;em&gt;is&lt;/em&gt; the language. The implementation level story is all there is.&lt;/li&gt;
      &lt;li&gt;As we understand the computational story of the brain … it’s incredibly messy and convoluted.
        &lt;ul&gt;
          &lt;li&gt;What I mean is that latent models and manifolds are tricky to parameterize and design in a way where they can a) be accurate in predicting states and b) be human-understandable.
            &lt;blockquote&gt;
              &lt;p&gt;Neil deGrasse Tyson. Aug 21, 2017&lt;/p&gt;
              &lt;blockquote&gt;
                &lt;p&gt;“The universe is under no obligation to make sense to you.”&lt;/p&gt;
              &lt;/blockquote&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Anyways, this blog seems to stand opposite of the aisle of a particular view in Jeff Hawkins’ &lt;a href=&quot;https://www.amazon.com/Intelligence-Understanding-Creation-Intelligent-Machines/dp/0805078533/ref=asc_df_0805078533/?tag=hyprod-20&amp;amp;linkCode=df0&amp;amp;hvadid=312128454859&amp;amp;hvpos=&amp;amp;hvnetw=g&amp;amp;hvrand=2933067035242335183&amp;amp;hvpone=&amp;amp;hvptwo=&amp;amp;hvqmt=&amp;amp;hvdev=c&amp;amp;hvdvcmdl=&amp;amp;hvlocint=&amp;amp;hvlocphy=9031201&amp;amp;hvtargid=pla-432166617256&amp;amp;psc=1&quot;&gt;On Intelligence&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;The view is this:
            &lt;blockquote&gt;
              &lt;p&gt;Paraphrased from Hawkins, J., &amp;amp; Blakeslee, S. (2004). On intelligence.&lt;/p&gt;
              &lt;blockquote&gt;
                &lt;p&gt;Imagine that aliens look at Earth and see a bunch of roads and they catalog every detail of each road and which one leads where etc. but they don’t really understand what the roads &lt;em&gt;mean&lt;/em&gt;. That is until they understand that humans couldn’t teleport places and need roads to move from one place to another in cars. Once this detail falls into place, the functionality begins to make a lot of sense.&lt;/p&gt;
              &lt;/blockquote&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
          &lt;li&gt;Obviously this is in defense that there are some intuitve explanations that can be delivered through human understable theory.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;As is often the case, I don’t think we’ll ever be able to ring a bell and declare one of these viewpoints the winner. In fact, I think that though they may stand on opposite sides of the aisle – to declare that they are at odds is even a stretch.&lt;/li&gt;
      &lt;li&gt;I think a more appropriate understanding between the “there is a simple theory and once we know it, things will make more sense” and the “there’s no magic at the bottom that’ll simplify things for our tiny brains” is that these are just two different flavors of possible answers that can be served to the multitude of questions about the brain.&lt;/li&gt;
      &lt;li&gt;And one final note. I really appreciate the Tal’s perspective in this blog because it’s a lot harder to take than Jeff’s in the quote. There are just some questions that can’t be answered. For example, the question &lt;em&gt;why&lt;/em&gt; are some people taller than others is just ill posed. There are a variety of reasons, not one causal explanation we can point to everytime.&lt;/li&gt;
      &lt;li&gt;We’ll probably never stop asking well-posed and ill-posed questions about neuroscience. Or maybe we can just abandon our posts and leave it to philosophers. Either way, time to make peace with that now.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;strong&gt;We can choose to grimace as we stare into the void, or we can have a good laugh.&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Citations
        &lt;ul&gt;
          &lt;li&gt;Hawkins, J., &amp;amp; Blakeslee, S. (2004). On intelligence. New York: Times Books.&lt;/li&gt;
          &lt;li&gt;Searle, John. “Chinese room argument.” Scholarpedia 4.8 (2009): 3100.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">If we already understood the brain, would we even know it? by Tal Yarkoni.</summary></entry><entry><title type="html">Review 2</title><link href="http://localhost:4000/reviews/review2/" rel="alternate" type="text/html" title="Review 2" /><published>2021-11-14T13:16:13-08:00</published><updated>2021-11-14T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review2</id><content type="html" xml:base="http://localhost:4000/reviews/review2/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.biorxiv.org/content/10.1101/2021.02.26.433027v1.abstract&quot;&gt;Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights
&lt;/a&gt; by &lt;a href=&quot;https://www.google.com/search?q=Ramin+Khajeh&amp;amp;oq=Ramin+Khajeh&amp;amp;aqs=chrome..69i57.82j0j7&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt;Ramin Khajeh&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=rcb8W94AAAAJ&amp;amp;hl=ca&quot;&gt;Francesco Fumarola&lt;/a&gt;, &lt;a href=&quot;https://zuckermaninstitute.columbia.edu/larry-f-abbott-phd&quot;&gt;Larry F. Abbott&lt;/a&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Citation: Khajeh, Ramin, Francesco Fumarola, and L. F. Abbott. “Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights.” bioRxiv (2021).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt; I am very interested in this part of computational neuroscience, but I’m a bit of a beginner to this topic. Some of the stuff here goes over my head, esp. the tractability of the gamma distribution in setting synaptic weights. If I got anything horribly wrong or you have any feedback please email me at zladd@berkeley.edu. Later I plan to add a feedback section using disqus at the botttom of each blog.&lt;/li&gt;
      &lt;li&gt;Intro
        &lt;ul&gt;
          &lt;li&gt;Neurons recieve a lot of excitatory input so there must be inhbitory balance. Some mechanism are cited as responsible for this:
            &lt;ul&gt;
              &lt;li&gt;Recurrent excitation&lt;/li&gt;
              &lt;li&gt;Recurrent inhibition&lt;/li&gt;
              &lt;li&gt;Feedfoward excitation&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;No evidence that feedforward excitaiton is particularly strong in these models.
            &lt;ul&gt;
              &lt;li&gt;This is definitely new to me.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;States that
            &lt;ul&gt;
              &lt;li&gt;synaptic weight distributions have variance around $\frac{1}{\sqrt{k}}$ where $K$ is node in-degree.&lt;/li&gt;
              &lt;li&gt;order of inhibitory input is around $\sqrt(K)$&lt;/li&gt;
              &lt;li&gt;and it’s generally cancelled by an excitatory input of equal magnitude, but they want to avoid this.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Suggests that without feedforward exc. input, there will be low firing rate but moves this evidence in argument for a sparser network, not a dense netowrk with low firing rate neurons.&lt;/li&gt;
          &lt;li&gt;Getting rid of feedforward exc. means that synaptic distirbutions lose variance, so they suggest remedying this by making synaptic strength vary with a mean of $\frac{1}{\sqrt{K}}$ but then vary with order $\frac{1}{\sqrt{K}}$ this means total variance is now $\frac{\sqrt{K}}{\sqrt{K}}$ again (prev. it was $\frac{1}{\sqrt{k}}$)&lt;/li&gt;
          &lt;li&gt;One part of this that I don’t understand is a formal mathematical definition for a &lt;em&gt;synaptic weight distribution&lt;/em&gt;. I think that for a more experienced reader, they will know what this means. My understanding is that this distribution will be a vector like $\bf{x} \in \mathbb{R^{K}}$ and this vector with the excitory input would follow something like $\bf{x_{1:\sqrt{K}}} \sim \mathcal{N}(\frac{1}{\sqrt{K}},1)$ (standard model –&amp;gt; variance == 1?) and the inihbitory component would be $\bf{x_{1:\sqrt{K}}} \sim \mathcal{N}(-\frac{1}{\sqrt{K}},1)$.
            &lt;ul&gt;
              &lt;li&gt;there are a few things I here I would want to ask:
                &lt;ol&gt;
                  &lt;li&gt;We have only accounted for $ 2 \times \sqrt{K}$ of the $K$ weights. Are the rest of the synaptic weights just 0?
                    &lt;ul&gt;
                      &lt;li&gt;honestly, I’m missing something here. I think all weights are accounted for, I just don’t know how.&lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;Do these weights have unit variance?&lt;/li&gt;
                &lt;/ol&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The model
        &lt;ul&gt;
          &lt;li&gt;This part starts to clear up the questions above&lt;/li&gt;
          &lt;li&gt;They state that
            &lt;blockquote&gt;
              &lt;p&gt;Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights. (Khajeh 2021)&lt;/p&gt;
              &lt;blockquote&gt;
                &lt;p&gt;Networks have currents $x_i$ for $i = 1 … N$ and firing rates $\phi(x_i)$ that obey
   \begin{equation}
   \Gamma_x \frac{dx_i}{dt} = -x_i - \sum\limits_j^N J_{ij}\phi(x_j) + I_0
   \end{equation} where $\phi$ is a nonlinear function and $J_{ij} \geq 0$&lt;/p&gt;
              &lt;/blockquote&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
          &lt;li&gt;I am not sure what $J_{ij}$ represents now. *note: it get’s explained a few sentences later. – they are i.i.d. weights.&lt;/li&gt;
          &lt;li&gt;Besides that, this function does a good job of clearly defining synaptic integration.&lt;/li&gt;
          &lt;li&gt;Considers tangenet vs. exponential nonlinear activation functions for $\phi$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Simulation results
        &lt;ul&gt;
          &lt;li&gt;When excitatory input is set to $I_0 \sim \sqrt{K}$ the network has either sparse and high firing rate reponses (high variance), or dense and low firing rate responses (low variance).
            &lt;ul&gt;
              &lt;li&gt;this lines up nicely with what was introduced in the beginning and has a very clear inuitive interpretation. Nice!&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Then they dig in to some detailed numerical explanations for this effect.&lt;/li&gt;
          &lt;li&gt;Figure 1 is very clear :thumbsup:
            &lt;ul&gt;
              &lt;li&gt;I was surprised to see that in panel G, the feedfoward input only had a small impact on the fraction of active neurons.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Sparse E/I balance lead to non-gaussian 0-skewed distribution of synaptic weights&lt;/li&gt;
          &lt;li&gt;The authors state that these results represent a robust distribution of synaptic weights desipte not having any excitatory input.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Analysis of Sparse Balance Networks
        &lt;ul&gt;
          &lt;li&gt;here the authors explore high variance, low exc. input, sparse networks further using heavside non-linear response function.
            &lt;ul&gt;
              &lt;li&gt;had to look this one up, but it’s basically a piecewise activaiton function where the gradient is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;dirac delta function&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;weights drawn from $\text{gamma}(k, \theta)$&lt;/li&gt;
          &lt;li&gt;Figure 3A would benefit from text titles on the figure, not just in the caption (at least for a reader like me lol I suck at reading figs).&lt;/li&gt;
          &lt;li&gt;What I got out of Fig. 3 is that heavside function lead to lower fraction of active neurons compared to tanh and exp. activation functions and thus a sparser netowrk. But that it also had a non-gaussian distribution of weights under the sparse condition.&lt;/li&gt;
          &lt;li&gt;Interesting point about having a larger $K$ or shape parameter for gamma, one might expect that the gamma function approximate gaussian function. But this is not the case with $\eta \sim \text{gamma}(\alpha, \theta)$.&lt;/li&gt;
          &lt;li&gt;I wonder if I understood that right?&lt;/li&gt;
          &lt;li&gt;If feedforward bias is small  –&amp;gt; large synaptic variance required for robust fluctuation.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Sparse Activity Arises from Network Dynamics
        &lt;ul&gt;
          &lt;li&gt;They use population averaged autocorrelation function to find out how much the weight distribution is affected by the previous weight distribution at some lag.&lt;/li&gt;
          &lt;li&gt;Autocorrelation decays faster for larger K (fig. 4).
            &lt;blockquote&gt;
              &lt;p&gt;Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights. (Khajeh 2021)&lt;/p&gt;
              &lt;blockquote&gt;
                &lt;p&gt;it is the dynamics of the recurrent synaptic inputs, not their size, that leads to sparse activity at large K&lt;/p&gt;
              &lt;/blockquote&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Mean Field Analysis
        &lt;ul&gt;
          &lt;li&gt;Introduces a variable $m$ which is the mean-field approximation for $\bar{\phi}$&lt;/li&gt;
          &lt;li&gt;Based on fuctions 7 and 11 (I think) they define closed form equation for $m$ in terms of gamma dist. shape parameter $\alpha$ as 
  \begin{equation}
  \alpha = \frac{mJ_0^2\sqrt(m)}{g^2}
  \end{equation}
            &lt;ul&gt;
              &lt;li&gt;$g^2$ comes from gamma dist. scale parameter.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;considers several cases of computing autocorrelation of $\eta$ based on different sizes of $K$ and decorrealtion rate $\beta$.
            &lt;ul&gt;
              &lt;li&gt;When $K$ is small, then $eta$ follows gamma distribution $\text{gamma}(\alpha, \theta)$ and then they can solve closed form mean field calcuation using integral.&lt;/li&gt;
              &lt;li&gt;even at $K ~ 10^3$ this holds.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Then there’s a more complicated closed form calculation for $var(\eta)$ that I’m gonna skip over for now.
            &lt;ul&gt;
              &lt;li&gt;involves decomposing the variance into quenched and time dependent.&lt;/li&gt;
              &lt;li&gt;not very familiar with this decomposition. Maybe if I look into mean field decomp. more I will see.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Fig. 5 just shows that this recover of gamma distribution parameters charcterizes weights and input parameter well. Again, clear and nice figure :thumbsup:&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Sparse balance in E-I network
        &lt;ul&gt;
          &lt;li&gt;shows that &lt;em&gt;network properties&lt;/em&gt; shown for I network model also apply for E/I model, such as sparsity (frac. of neurons active) and firing rates.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Discussion
        &lt;ul&gt;
          &lt;li&gt;Shows interesting network properties demonstrated in this anaylsis as a result of making feedforward excitation smaller.&lt;/li&gt;
          &lt;li&gt;Broadens out implications to real world instances where E/I ratio is similarly composed.&lt;/li&gt;
          &lt;li&gt;Interesting to point out this phenomenon in relating autocorrelation and network sparse responses seems to be a general effect.&lt;/li&gt;
          &lt;li&gt;Definitely incresed my interested in mean field theory.&lt;/li&gt;
          &lt;li&gt;Here is what I think is really the most fascinating takeway:
  &lt;strong&gt;“The large degree of variability in the synapses could route stimulus information along particular  paths across network neurons”&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;A reasonable claim that it is not just feedforward input that influences selective network responses but also recurrent synaptic input.
            &lt;ul&gt;
              &lt;li&gt;The number of recurrent connections in the brain would support this idea. If there is so many recurrent networks, they must be doing something.&lt;/li&gt;
              &lt;li&gt;Ideas here build on some early work by M. Jordan cited in The Computational Brain by Terrence Sejnowski on using recurrent networks to identify shapes and the expressive power of &lt;em&gt;non-feedforward&lt;/em&gt; connections.&lt;/li&gt;
              &lt;li&gt;Granted, I haven’t actually read this 1998 work, just read the watered down example in Dr. Sejnowski’s book.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights by Ramin Khajeh, Francesco Fumarola, Larry F. Abbott</summary></entry><entry><title type="html">Review 1</title><link href="http://localhost:4000/reviews/review1/" rel="alternate" type="text/html" title="Review 1" /><published>2021-11-14T13:16:13-08:00</published><updated>2021-11-14T13:16:13-08:00</updated><id>http://localhost:4000/reviews/review1</id><content type="html" xml:base="http://localhost:4000/reviews/review1/">&lt;ul&gt;
  &lt;li&gt;Review 1 : &lt;a href=&quot;https://www.eneuro.org/content/8/2/ENEURO.0131-21.2021&quot;&gt;In Silico&lt;/a&gt; by &lt;a href=&quot;https://fairhalllab.com/&quot; title=&quot;Title&quot;&gt;Dr. Fairhall&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Citation: Fairhall, Adrienne L. “In silico: where next?.” &lt;em&gt;Eneuro&lt;/em&gt; 8.2 (2021).&lt;/li&gt;
      &lt;li&gt;Intro
        &lt;ul&gt;
          &lt;li&gt;This writing starts out by laying out the context of 2013, a very important year for funding large neuroscience research initiatives, the U.S. BRAIN projects and the European Human Brain Project (HBP).&lt;/li&gt;
          &lt;li&gt;Furthers discussion into brain atlases from Allen institute and recent developments in imaging and recording techniques.&lt;/li&gt;
          &lt;li&gt;Mentioned a paper I found interested on neural networks as simulation tools : &lt;a href=&quot;https://pubmed.ncbi.nlm.nih.gov/28668365/&quot;&gt;Barak 2007&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Key Question: “On this backdrop, what are valuable targets for future
large-scale spending?”
        &lt;ul&gt;
          &lt;li&gt;To me, this seems to be the transition from historical background to the discussion of what next.&lt;/li&gt;
          &lt;li&gt;And here Dr. Fairhall points out a key issue facing large scale collaboration in brain science: What big experiments and datasets will lead to transformative research?&lt;/li&gt;
          &lt;li&gt;The answer being, there’s not really some resounding agreement.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Possible answers
        &lt;ul&gt;
          &lt;li&gt;Dr. Abbot suggests the approach of a deatiled cellular model of a mouse brain, however, as HBP has experienced, there is a challenge here with integrating biophysical modeling with levels of connectivity (gap junction, neuromodulation, etc.)&lt;/li&gt;
          &lt;li&gt;An alternative approach is discussed as a brain activity map that bridges existing theories about different regions of the brain in modeling one activity in order to align existsing theories.
            &lt;ul&gt;
              &lt;li&gt;The challenge here seems to be design. How does one actually assemble all these odds and ends.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;I thought this quote was interesting:
            &lt;blockquote&gt;
              &lt;p&gt;In Silico (Fairhall 2021)&lt;/p&gt;
              &lt;blockquote&gt;
                &lt;p&gt;“IARPA’s MICrONS program is […] a collaborative effort to reconstruct the connectome of a section of visual cortex whose activity has been characterized in vivo. By establishing the functional relevance of specific neurons, this project addresses some of the criticisms made of both the Blue Brain (Yong, 2019) and connectomics approaches.”&lt;/p&gt;
              &lt;/blockquote&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;I thought it was interesting for a few reasons. First, I wonder what the actual criticms are in the cited paper, I hear about criticsms all the time but yet I don’t quite know what they are. Second, I wonder how this project manages to address such criticsms. An easy guess is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;function relevance&lt;/code&gt; suggests this project is doing more to link modeling with functionality … but this is still vague in my mind,&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;Finally, mention of a collaborative study where different groups measure populations of 1000 neurons.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Now that the author has introduced the problem and several approaches. it’s time to conclude and leave the reader feeling warm and fuzzy that we will solve all the problems in neuroscience. Kidding of course.
        &lt;ul&gt;
          &lt;li&gt;Author notes that projects that produce a “product” such as Allen datasest are less risky than projects that aim to tackle large problems.&lt;/li&gt;
          &lt;li&gt;Suggests diversifying research investment across labs.&lt;/li&gt;
          &lt;li&gt;Then model of brain observatories gathering data on a large scale and then smaller labs performing analysis on these dataset.
            &lt;ul&gt;
              &lt;li&gt;The nice part about this is that the datasets will be well vetted by various research groups.&lt;/li&gt;
              &lt;li&gt;And computational neuroscientists can save some money on having to run large scale experiments – they get nice data.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;We know a lot but we know nothing.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;The last line here really hits home. There is a massive body of research on neuroscience, yet unifying theories (across levels of complexity) are few and far between.&lt;/li&gt;
      &lt;li&gt;Overall, this is a great paper to introduce some of the key questions facing large collaborative neuroscience and was a really nice read for my first review. I can imagine I’ll be reading more papers with a lot of narrative gating, but this was not that. It was straight the point and clear. Sure some parts were not elaborated on, but the citations are there and I can follow the trail. I believe this is a balance one has to strike in order to deliver a clear message and something I hope to take with me in my own writing.&lt;/li&gt;
      &lt;li&gt;Good stuff :) thanks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>xander</name></author><category term="reviews" /><summary type="html">Review 1 : In Silico by Dr. Fairhall Citation: Fairhall, Adrienne L. “In silico: where next?.” Eneuro 8.2 (2021). Intro This writing starts out by laying out the context of 2013, a very important year for funding large neuroscience research initiatives, the U.S. BRAIN projects and the European Human Brain Project (HBP). Furthers discussion into brain atlases from Allen institute and recent developments in imaging and recording techniques. Mentioned a paper I found interested on neural networks as simulation tools : Barak 2007 Key Question: “On this backdrop, what are valuable targets for future large-scale spending?” To me, this seems to be the transition from historical background to the discussion of what next. And here Dr. Fairhall points out a key issue facing large scale collaboration in brain science: What big experiments and datasets will lead to transformative research? The answer being, there’s not really some resounding agreement. Possible answers Dr. Abbot suggests the approach of a deatiled cellular model of a mouse brain, however, as HBP has experienced, there is a challenge here with integrating biophysical modeling with levels of connectivity (gap junction, neuromodulation, etc.) An alternative approach is discussed as a brain activity map that bridges existing theories about different regions of the brain in modeling one activity in order to align existsing theories. The challenge here seems to be design. How does one actually assemble all these odds and ends. I thought this quote was interesting: In Silico (Fairhall 2021) “IARPA’s MICrONS program is […] a collaborative effort to reconstruct the connectome of a section of visual cortex whose activity has been characterized in vivo. By establishing the functional relevance of specific neurons, this project addresses some of the criticisms made of both the Blue Brain (Yong, 2019) and connectomics approaches.”</summary></entry></feed>