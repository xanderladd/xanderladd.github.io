<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning - Alexander Ladd</title>
<meta name="description" content="The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Alexander Ladd">
<meta property="og:title" content="Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning">
<meta property="og:url" content="http://localhost:4000/reviews/review9/">


  <meta property="og:description" content="The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang">







  <meta property="article:published_time" content="2022-01-09T04:16:13-08:00">





  

  


<link rel="canonical" href="http://localhost:4000/reviews/review9/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Alexander Ladd",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Alexander Ladd Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--base wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/xander_pic.png" alt="Alexander Ladd"></a>
        
        <a class="site-title" href="/">
          Alexander Ladd
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about">about</a>
            </li><li class="masthead__menu-item">
              <a href="/projects">projects</a>
            </li><li class="masthead__menu-item">
              <a href="/research">research</a>
            </li><li class="masthead__menu-item">
              <a href="/reviews">reviews</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url"></a>
    </h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</h1>
    
    <p><a href="https://arxiv.org/pdf/2109.02869.pdf">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</a> by <a href="https://otoro.net/ml/">Dr. Ha</a> and <a href="https://www.linkedin.com/in/yujin-tang-98b3ab5a/?originalSubdomain=jp">Yujin Tang</a></p>

<ul>
  <li>
    <p>Citation: Tang, Yujin, and David Ha. “The sensory neuron as a transformer: Permutation-invariant neural networks for reinforcement learning.” Advances in Neural Information Processing Systems 34 (2021).</p>
  </li>
  <li>Today, I am reading a paper in NeurIPS by Dr. David Ha and Dr. Yujin Tang.</li>
  <li>These reviews are not really reviews but more like casual reading notes. I am writing what pops into my head as I read.</li>
  <li>The first thing that stands out to me just from reading the introduction is that shuffling around sensory input is generally fatal for most network problems. I doubt most image recognition modules would work at all if one just shuffled around 4x4 pixel blocks in an image.
    <ul>
      <li>I can think of a few problems that begin to crop up here.</li>
      <li>Firstly, if we think about swapping the pixels in a picture of a cat one by one, how many swaps do we need to make until it is just not a picture of a cat.
        <ul>
          <li>This issue is rectified in the paper because they are trying to complete a task or play a game, which means the objective is still definitively the same regardless of input shuffling.</li>
        </ul>
      </li>
      <li>Secondly, how does one parameterize image shuffling? Say we decide to shuffle around $64 x 64$ squares in an image. If we know the image is $256 x 256$ then it makes sense to define $16$ networks. But we’ve cheated and used some a priori knowledge about the image permutations.
        <ul>
          <li>$\frac{256 \times 256}{64 \times 64} = 16$</li>
        </ul>
      </li>
      <li>But now I start getting ahead of myself, I’ll keep reading and perhaps these issues aren’t relevant. Onward!</li>
    </ul>
  </li>
  <li>One more thing in the introduction is really surprising/exciting about this. All these references to cellular automata (CA) and emergent behavior makes me really excited to see how these local networks are going to start forming a coherent policy.</li>
  <li>Self-organizing network agents, while being a mouthful, does seem like a natural progression. I’m having trouble explaining why. I wanted to write something about NNs working better on local computation, but they do seem to do global computation pretty well too. I’m not sure this is the right distinction to make (whether NNs are better as local agents or singular global agents). Also there’s so much more research for the latter.</li>
  <li>Meta-learning policies
    <ul>
      <li><strong>fast weights</strong>: adapt weights to input.</li>
      <li><strong>associative weights</strong>: allow RNN weights to be attracted to recent hidden states.</li>
      <li><strong>hypernetworks</strong>: one network generates the weights for another.</li>
      <li><strong>Hebbian learning</strong>: See more <a href="https://arxiv.org/abs/2002.10585">here</a></li>
    </ul>
  </li>
  <li>Attention
    <ul>
      <li>similar to previous adaptive weight mechanisms in that it modifies weights based on inputs.</li>
      <li>Authors cite several examples of attention learning inductive biases.</li>
    </ul>
  </li>
  <li>Method
    <ul>
      <li>A legit permutation invariant (PI) agent doesn’t need to be trained on permutations to recognize them.</li>
      <li>PI formulation: need a function $f(x): \mathcal{R}^N \longrightarrow \mathcal{R}^M$ s.t. $ f(x[x]) f(x)$ where $s$ is some permutation of the indices ${1 … n}$</li>
      <li>Self-attention as PE.
        <ul>
          <li>Simplest self attention: $\sigma(QK^T)V$ where $Q,K \in \mathcal{R^{n\times d_{q}}}$ and $V \in \mathcal{R^{n\times d_{v}}}$. Q,V and K are query, value and key matrices respectively.</li>
          <li>Normally these (Q,K,V) are functions of the input.</li>
        </ul>
      </li>
      <li>contribution
        <ul>
          <li>authors propose to add an extra layer (called AttentionNeuron) in front of agent’s policy network $\pi$ that accepts observation $o_t$ and previous action $a_{t-1}$ as inputs.</li>
          <li>The $i$th neuron only has access to $o_t[i]$.</li>
          <li>each sensory neuron computes messages $f_k(o_t[i], a_{t-1})$ and $f_v(o_t[i])$.</li>
          <li>These messages are aggregated in <em>global latent code</em> $m_t$.</li>
        </ul>
      </li>
      <li>So I think it’s interesting that based off the figure, they don’t model this AttentionNeuron layer as a distinct part of each neuron but rather a layer that receives input from each neuron and aggregates input into $m_t$.
        <ul>
          <li>Computationally, it doesn’t matter if the transformation is affixed to the last layer of each neuron or if it stands alone. But I thought it was illustrative of how the Query, Key and Value matrices are formed.</li>
        </ul>
      </li>
      <li>Key matrix depends on $f_k(o_t, a_{t-1})$ and value matrix depends on $f_v(o_t)$</li>
      <li>They also have projection matrices $W_k, W_v, W_q$ (which is what gives us PE).</li>
      <li>Experiments and observation space are
        <ul>
          <li>Cartpole $\mathcal R^5$</li>
          <li>Ant $\mathcal R^28$</li>
          <li>Car Racing $\mathcal R^{96x96x4}$</li>
          <li>Atari Pong $\mathcal R^{84x84x4}$
            <ul>
              <li>Car Racing and Atari Pong have dimension $4$ because they are stacked grayscale RGB values.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>They never use more than 196 activation neurons.</li>
      <li>Also activation neurons do seem to be a function of observation space.</li>
      <li>Action space ranges from 1 action, to 8 possible actions.</li>
    </ul>
  </li>
  <li>Brief discussion of design choices.
    <ul>
      <li>I wonder what the motivation is for making $QW_q$ learnable instead of input dependent.</li>
    </ul>
  </li>
  <li>They didn’t use a projection matrix for the Value matrix ($V$)</li>
  <li>The don’t use RNNs for high dimensional input, just feed forward neural network (FFN).</li>
  <li>Results
    <ul>
      <li>CartPoleSwingUpHarder
        <ul>
          <li>They compare against a two layer FFN trained with CMA-ES</li>
          <li>The benchmark agent is able to balance the pole faster under normal circumstances because their method requires a few burn in steps.</li>
          <li>But what is really notable here is that various peterbutations of shuffling the input or concatenating a noisy vector to the input don’t affect performance. No retraining required!
            <ul>
              <li>Might’ve liked to see a comparison against retrained benchmark.</li>
              <li>Yes I know it’s comparing apples to oranges and it probably wouldn’t make sense to put in a paper … but I’m curious.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>PyBullet Ant
        <ul>
          <li>Uses pre-trained models and behavior cloning for the benchmark.</li>
          <li>From table 4 the most notable insights I picked up:
            <ul>
              <li>BC works better with larger subsequent layers.</li>
              <li>ES shuffled works well on their agent as is. Slightly underperforms non-shuffled input version (FFN teacher). Shuffled FFN gets a really bad score.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Atari Pong
        <ul>
          <li>** shuffled atari pong :)</li>
          <li>I like the idea of maintaining a spatial representation of the 2D grid in $m_t$ because it is more interpretable.</li>
          <li>I see that this agent can take a subset of the screen and the authors suggest that would be interesting to conduct some kind of ablation experiment on. While I agree, I am also interested in what would happen if you blacked out part of the screen, thus forcing the agent to hallucinate what is happening in those shuffled blocks.</li>
          <li>The occluded agent rarely won when it couldn’t see certain blocks, but when the blocks are added back it does well.</li>
          <li>This is the generalization advantage.</li>
          <li>Figure 6 is a great example of why keeping 2d spatial embeddings is a nice design. The separability of embedding states based on state space is cool to look at.
            <ul>
              <li>Sometimes when I read these papers, I wonder how they coded some things in. This is one I am particularly interested on.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Car racing
        <ul>
          <li>As I guessed in the beginning, attention-based mechanisms are crippled by shuffling inputs.
            <ul>
              <li>This is shown in AttentionAgent failure to do anything but an original task.</li>
            </ul>
          </li>
          <li>I have no idea how netRand + AttentionAgent are combined and I don’t understand them pretty well.</li>
          <li>That said, it’s clear that is the only way to compete with the presented method given new unseen and shuffled racing maps.</li>
          <li>Ah I spoke too soon… it’s explained below the figure.</li>
          <li>I see they just add AttentionAgent layer at the end but unfortunately, I don’t understand enough about these methods to say anything else.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Discussion + Conclusion
    <ul>
      <li>Further uses included dynamic input-output mappings in robots or cross-wired systems.
        <ul>
          <li>This isn’t an entirely broad application – it’s a bit specific to systems that receive shuffled inputs.</li>
        </ul>
      </li>
      <li>However, I think the most important impact of this paper is that it “provides a lens for understanding a transformer as a self-organizing network” in what I felt was a natural way.
        <ul>
          <li>When I read the title, I imagined the authors would need to make more substantial leaps in connecting transformers with a sensory neuron, but now that I’ve read the paper it feels ok.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>One thing I wonder when reading this paper is: what is the best way to aggregate signals from the individual agents? This paper looked at aggregationthrough neural network layers, but I thought the reshape in the 2D spatial embeddingswas an interesting twist.
    <ul>
      <li>Generally, this paper adds some interesting ingredients to a mixture of local -&gt; global computation using distributed agents.</li>
    </ul>
  </li>
  <li>Another thing I really like about this paper is that it isn’t crazy hard to understand. So many ML papers are steeped in lots of formula and notation and this paper truly tried to clearly communicate the message.</li>
</ul>





<!-- 
<div class="entries-list">
  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review28/" rel="permalink">Review 28: Four ethical priorities for neurotechnologies and AI
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Four ethical priorities for neurotechnologies and AI
 by Rafael Yuste and Sara Goering

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review27/" rel="permalink">Review 27: Parsing learning in networks using brain–machine interfaces
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Parsing learning in networks using brain-machine
interfaces by Amy Orsborn and Bijan Pesaran

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review26/" rel="permalink">Review 26: A control-theoretic approach to brain-computer interface design
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A control-theoretic approach to brain-computer interface design by Yin Zhang and Steven Chase

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review25/" rel="permalink">Review 25: Plug-and-play control of a brain-computer interface through neural map stabilization
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Plug-and-play control of a brain-computer interface through neural map stabilization
 by Daniel Silversmith.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review24/" rel="permalink">Review 24:The brain-reading devices helping paralysed people to move, talk and touch
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The brain-reading devices helping paralysed people to move, talk and touch
 by Liam Drew.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review23/" rel="permalink">Review 23: Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Neuropixels 2.0: A miniaturized high-density probe for stable, long-term
brain recordings by Nick Steinmetz.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review21/" rel="permalink">Review 21: Cracking the Neural Code in Humans
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Cracking the Neural Code in Humans
 by Emily Singer at the Simons Foundation.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review20/" rel="permalink">Review 20: Trends in Computational Neuroscience: Cosyne 2022
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Trends in Computational Neuroscience: Cosyne 2022
 by Sabera Talukder

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review19/" rel="permalink">Review 19: Let us never speak of these values again.
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Let us never speak of these values again. by Ben Recht

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review18/" rel="permalink">Review 18: Deep Learning Is Hitting a Wall
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Deep Learning is Hitting a Wall by Gary Marcus

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review17/" rel="permalink">Review 17: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 3
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Predictive Coding, Variational Autoencoders,
and Biological Connections by Joseph Marino

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review16/" rel="permalink">Review 16: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Predictive Coding, Variational Autoencoders,
and Biological Connections by Joseph Marino

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review15/" rel="permalink">Review 15: Predictive Coding, Variational Autoencoders, and Biological Connections Pt 1
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Predictive Coding, Variational Autoencoders,
and Biological Connections by Joseph Marino

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review14/" rel="permalink">Review 14: Computation Through Neural Population Dynamics Part 2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Computation Through Neural Population Dynamics by Saurabh Vyas

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review13/" rel="permalink">Review 13: Computation Through Neural Population Dynamics Part 1
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Computation Through Neural Population Dynamics by Saurabh Vyas

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review12/" rel="permalink">Review 12: Just Ask for Generalization
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Just Ask for Generalization by Eric Jang

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review11/" rel="permalink">Review 11: The Bandwagon
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Bandwagon by Claude E. Shannon

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review10/" rel="permalink">Review 10: Beyond advertising: New infrastructures for publishing integrated research objects
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Beyond advertising: New infrastructures for publishing integrated research objects by Elizabeth DuPre et al.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review9/" rel="permalink">Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review8/" rel="permalink">Review 8: The Future of Artificial Intelligence is Self-Organizing and Self-Assembling
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Future of Artificial Intelligence is Self-Organizing and Self-Assembling by Prof. Sebastian Risi

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review7/" rel="permalink">Review 7: Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors by Dr. Austin Graves Dr. Richard Huganir.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review6/" rel="permalink">Review 6: Spiking Neural Nets
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Spiking Neural Networks by Anil Ananthaswamy.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review5/" rel="permalink">Review 5
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Making RL Tractable by Learning More Informative Reward Functions: Example-Based Control, Meta-Learning, and Normalized Maximum Likelihood.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/Not_Even_Wrong/" rel="permalink">Not Even Wrong
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Wow, I just found out what it means to be “Not even wrong”. The phase, coined by Wolfgang Pauli has just given me a huge slice of humble pie. Basically not e...</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review4/" rel="permalink">Review 4
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  If we already understood the brain, would we even know it? by Tal Yarkoni.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review3/" rel="permalink">Review 3
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  From synapse to network: models of information storage and retrieval in neural circuits
 by Johnatan (Yonatan) Aljadeff et al.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review2/" rel="permalink">Review 2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights
 by Ramin Khajeh, Francesco Fumarola, Lar...</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review1/" rel="permalink">Review 1
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Review 1 : In Silico by Dr. Fairhall
    
      Citation: Fairhall, Adrienne L. “In silico: where next?.” Eneuro 8.2 (2021).
      Intro
        
        ...</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/whos_in_charge/" rel="permalink">Who’s In Charge
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Well I am very busy so I’ll keep this short and sweet but I was listening to Alan Watts and ever since I’ve turned it off this one thought has stuck with me....</p>
  </article>
</div>

  
</div> -->

<!-- 

<title>Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning - Alexander Ladd</title>
<meta name="viewport" content="width=device-width">

<link rel="stylesheet" href="/css/all.css"> -->
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-51807848-1', 'simple.gy');
  ga('send', 'pageview');
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS",
      tags: 'ams'
      },
      tex: {packages: {'[+]': ['boldsymbol'], '[+]': ['bm']}}

    }
  });
</script>



<script src="https://utteranc.es/client.js"
        repo="xanderladd/xanderladd.github.io"
        issue-term="pathname"
        label="comments"
        theme="photon-dark"
        crossorigin="anonymous"
        async>
</script>




  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Alexander Ladd. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', '');
    script.setAttribute('issue-term', 'pathname');
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





  </body>
</html>
