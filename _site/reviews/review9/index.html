<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning - Alexander Ladd</title>
<meta name="description" content="The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang">



<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Alexander Ladd">
<meta property="og:title" content="Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning">
<meta property="og:url" content="http://localhost:4000/reviews/review9/">


  <meta property="og:description" content="The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang">







  <meta property="article:published_time" content="2022-01-09T04:16:13-08:00">





  

  


<link rel="canonical" href="http://localhost:4000/reviews/review9/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Alexander Ladd",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Alexander Ladd Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--base wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/xander_pic.png" alt="Alexander Ladd"></a>
        
        <a class="site-title" href="/">
          Alexander Ladd
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about">about</a>
            </li><li class="masthead__menu-item">
              <a href="/projects">projects</a>
            </li><li class="masthead__menu-item">
              <a href="/research">research</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url"></a>
    </h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</h1>
    
    <p><a href="https://arxiv.org/pdf/2109.02869.pdf">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning</a> by <a href="https://otoro.net/ml/">Dr. Ha</a> and <a href="https://www.linkedin.com/in/yujin-tang-98b3ab5a/?originalSubdomain=jp">Yujin Tang</a></p>

<ul>
  <li>
    <p>Citation: Tang, Yujin, and David Ha. “The sensory neuron as a transformer: Permutation-invariant neural networks for reinforcement learning.” Advances in Neural Information Processing Systems 34 (2021).</p>
  </li>
  <li>Today, I am reading a paper in NeurIPS by Dr. David Ha and Dr. Yujin Tang.</li>
  <li>These reviews are not really reviews but more like casual reading notes. I am writing what pops into my head as I read.</li>
  <li>The first thing that stands out to me just from reading the introduction is that shuffling around sensory input is generally fatal for most network problems. I doubt most image recognition modules would work at all if one just shuffled around 4x4 pixel blocks in an image.
    <ul>
      <li>I can think of a few problems that begin to crop up here.</li>
      <li>Firstly, if we think about swapping the pixels in a picture of a cat one by one, how many swaps do we need to make until it is just not a picture of a cat.
        <ul>
          <li>This issue is rectified in the paper because they are trying to complete a task or play a game, which means the objective is still definitively the same regardless of input shuffling.</li>
        </ul>
      </li>
      <li>Secondly, how does one parameterize image shuffling? Say we decide to shuffle around $64 x 64$ squares in an image. If we know the image is $256 x 256$ then it makes sense to define $16$ networks. But we’ve cheated and used some a priori knowledge about the image permutations.
        <ul>
          <li>$\frac{256 \times 256}{64 \times 64} = 16$</li>
        </ul>
      </li>
      <li>But now I start getting ahead of myself, I’ll keep reading and perhaps these issues aren’t relevant. Onward!</li>
    </ul>
  </li>
  <li>One more thing in the introduction is really surprising/exciting about this. All these references to cellular automata (CA) and emergent behavior makes me really excited to see how these local networks are going to start forming a coherent policy.</li>
  <li>Self-organizing network agents, while being a mouthful, does seem like a natural progression. I’m having trouble explaining why. I wanted to write something about NNs working better on local computation, but they do seem to do global computation pretty well too. I’m not sure this is the right distinction to make (whether NNs are better as local agents or singular global agents). Also there’s so much more research for the latter.</li>
  <li>Meta-learning policies
    <ul>
      <li><strong>fast weights</strong>: adapt weights to input.</li>
      <li><strong>associative weights</strong>: allow RNN weights to be attracted to recent hidden states.</li>
      <li><strong>hypernetworks</strong>: one network generates the weights for another.</li>
      <li><strong>Hebbian learning</strong>: See more <a href="https://arxiv.org/abs/2002.10585">here</a></li>
    </ul>
  </li>
  <li>Attention
    <ul>
      <li>similar to previous adaptive weight mechanisms in that it modifies weights based on inputs.</li>
      <li>Authors cite several examples of attention learning inductive biases.</li>
    </ul>
  </li>
  <li>Method
    <ul>
      <li>A legit permutation invariant (PI) agent doesn’t need to be trained on permutations to recognize them.</li>
      <li>PI formulation: need a function $f(x): \mathcal{R}^N \longrightarrow \mathcal{R}^M$ s.t. $ f(x[x]) f(x)$ where $s$ is some permutation of the indices ${1 … n}$</li>
      <li>Self-attention as PE.
        <ul>
          <li>Simplest self attention: $\sigma(QK^T)V$ where $Q,K \in \mathcal{R^{n\times d_{q}}}$ and $V \in \mathcal{R^{n\times d_{v}}}$. Q,V and K are query, value and key matrices respectively.</li>
          <li>Normally these (Q,K,V) are functions of the input.</li>
        </ul>
      </li>
      <li>contribution
        <ul>
          <li>authors propose to add an extra layer (called AttentionNeuron) in front of agent’s policy network $\pi$ that accepts observation $o_t$ and previous action $a_{t-1}$ as inputs.</li>
          <li>The $i$th neuron only has access to $o_t[i]$.</li>
          <li>each sensory neuron computes messages $f_k(o_t[i], a_{t-1})$ and $f_v(o_t[i])$.</li>
          <li>These messages are aggregrated in <em>global latent code</em> $m_t$.</li>
        </ul>
      </li>
      <li>So I think it’s interesting that based off the figure, they don’t model this AttentionNeuron layer as distinct part of each neuron but rather a layer that recives input from each neuron and aggregates input into $m_t$.
        <ul>
          <li>Computationally, it doesn’t matter if the transformation is affixed to the last layer of each neuron or if it stands alone. But I thought it was illustative of how the Query, Key and Value matrices are formed.</li>
        </ul>
      </li>
      <li>Key matrix depends on $f_k(o_t, a_{t-1})$ and value matrix depends on $f_v(o_t)$</li>
      <li>They also have projection matrices $W_k, W_v, W_q$ (which is what gives us PE).</li>
      <li>Experiments and observation space are
        <ul>
          <li>Cartpole $\mathcal R^5$</li>
          <li>Ant $\mathcal R^28$</li>
          <li>Car Racing $\mathcal R^{96x96x4}$</li>
          <li>Atari Pong $\mathcal R^{84x84x4}$
            <ul>
              <li>Car Racing and Atari Pong have dimension $4$ because they are stacked grayscale RGB values.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>They never use more than 196 activation neurons.</li>
      <li>Also activation neurons do seem to be a function of observation space.</li>
      <li>Action space ranges from 1 action, to 8 possible actions.</li>
    </ul>
  </li>
  <li>Brief discussion of design choices.
    <ul>
      <li>I wonder what the motivation is for making $QW_q$ learnable instead of input dependent.</li>
    </ul>
  </li>
  <li>They didn’t use a projection matrix for the Value matrix ($V$)</li>
  <li>The don’t use RNNs for high dimensional input, just feed forward neural network (FFN).</li>
  <li>Results
    <ul>
      <li>CartPoleSwingUpHarder
        <ul>
          <li>They compare against a two layer FFN trained with CMA-ES</li>
          <li>The benchmark agent is able to balance the pole faster under normal circumstances because their method requires a few burn in steps.</li>
          <li>But what is really notable here is that various peterbutations of shuffling the input or concatenating a noisy vector to the input don’t affect performance. No retraining required!
            <ul>
              <li>Might’ve liked to see a comparison against retrained benchmark.</li>
              <li>Yes I know it’s comparing apples to oranges and it probably wouldn’t make sense to put in a paper … but I’m curious.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>PyBullet Ant
        <ul>
          <li>Uses pre-trained models and behavior cloning for the benchmark.</li>
          <li>From table 4 the most notable insights I picked up:
            <ul>
              <li>BC works better with larger subsequent layers.</li>
              <li>ES shuffled works well on their agent as is. Slightly underperforms non-shuffled input version (FFN teacher). Shuffled FFN gets a really bad score.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Atari Pong
        <ul>
          <li>** shuffled atari pong :)</li>
          <li>I like the idea of maintaining a spatial representation of the 2D grid in $m_t$ because it is more interpretable.</li>
          <li>I see that this agent can take a subset of the screen and the authors suggest that would be interesting to conduct some kind of ablation experiment on. While I agree, I am also interested in what would happen if you blacked out part of the screen, thus forcing the agent to hallucinate what is happening in those shuffled blocks.</li>
          <li>The occluded agent rarely won when it couldn’t see certain blocks, but when the blocks are added back it does well.</li>
          <li>This is the generalization advantage.</li>
          <li>Figure 6 is a great example of why keeping 2d spatial embeddings is a nice design. The separability of embedding states based on state space is cool to look at.
            <ul>
              <li>Sometimes when I read these papers, I wonder how they coded some things in. This is one I am particularly interested on.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>Car racing
        <ul>
          <li>As I guessed in the beginning, attention-based mechanisms are crippled by shuffling inputs.
            <ul>
              <li>This is shown in AttentionAgent failure to do anything but an original task.</li>
            </ul>
          </li>
          <li>I have no idea how netRand + AttentionAgent are combined and I don’t understand them pretty well.</li>
          <li>That said, it’s clear that is the only way to compete with the presented method given new unseen and shuffled racing maps.</li>
          <li>Ah I spoke too soon… it’s explained below the figure.</li>
          <li>I see they just add AttentionAgent layer at the end but unfortunately, I don’t understand enough about these methods to say anything else.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Discussion + Conclusion
    <ul>
      <li>Further uses included dynamic input-output mappings in robots or cross-wired systems.
        <ul>
          <li>This isn’t an entirely broad application – it’s a bit specific to systems that receive shuffled inputs.</li>
        </ul>
      </li>
      <li>However, I think the most important impact of this paper is that it “provides a lens for understanding a transformer as a self-organizing network” in what I felt was a natural way.
        <ul>
          <li>When I read the title, I imagined the authors would need to make more substantial leaps in connecting transformers with a sensory neuron, but now that I’ve read the paper it feels ok.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>One thing I wonder when reading this paper is: what is the best way to aggregate signals from the individual agents? This paper looked at aggrregation through neural network layers, but I thought the reshape in the 2D spatial embedddings was an interesting twist.
    <ul>
      <li>Generally, this paper adds some interesting ingredients to a mixture of local -&gt; global computation using distributed agents.</li>
    </ul>
  </li>
</ul>





<!-- 
<div class="entries-list">
  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review9/" rel="permalink">Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning by Dr. Ha and Yujin Tang

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review8/" rel="permalink">Review 8: The Future of Artificial Intelligence is Self-Organizing and Self-Assembling
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">The Future of Artificial Intelligence is Self-Organizing and Self-Assembling by Prof. Sebastian Risi

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review7/" rel="permalink">Review 7: Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Visualizing synaptic plasticity in vivo by large-scale imaging of endogenous AMPA receptors by Dr. Austin Graves Dr. Richard Huganir.

</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review6/" rel="permalink">Review 6: Spiking Neural Nets
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Spiking Neural Networks by Anil Ananthaswamy.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review5/" rel="permalink">Review 5
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Making RL Tractable by Learning More Informative Reward Functions: Example-Based Control, Meta-Learning, and Normalized Maximum Likelihood.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/Not_Even_Wrong/" rel="permalink">Not Even Wrong
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Wow, I just found out what it means to be “Not even wrong”. The phase, coined by Wolfgang Pauli has just given me a huge slice of humble pie. Basically not e...</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review4/" rel="permalink">Review 4
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  If we already understood the brain, would we even know it? by Tal Yarkoni.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review3/" rel="permalink">Review 3
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  From synapse to network: models of information storage and retrieval in neural circuits
 by Johnatan (Yonatan) Aljadeff et al.


</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review2/" rel="permalink">Review 2
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Sparse balance: excitatory-inhibitory networks with small bias currents and broadly distributed synaptic weights
 by Ramin Khajeh, Francesco Fumarola, Lar...</p>
  </article>
</div>

  
    



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/reviews/review1/" rel="permalink">Review 1
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
  Review 1 : In Silico by Dr. Fairhall
    
      Citation: Fairhall, Adrienne L. “In silico: where next?.” Eneuro 8.2 (2021).
      Intro
        
        ...</p>
  </article>
</div>

  
</div> -->

<!-- 

<title>Review 9: The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning - Alexander Ladd</title>
<meta name="viewport" content="width=device-width">

<link rel="stylesheet" href="/css/all.css"> -->
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-51807848-1', 'simple.gy');
  ga('send', 'pageview');
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true},
      jax: ["input/TeX","input/MathML","input/AsciiMath","output/CommonHTML"],
      extensions: ["tex2jax.js","mml2jax.js","asciimath2jax.js","MathMenu.js","MathZoom.js","AssistiveMML.js", "[Contrib]/a11y/accessibility-menu.js"],
      TeX: {
      extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"],
      equationNumbers: {
      autoNumber: "AMS",
      tags: 'ams'
      },
      tex: {packages: {'[+]': ['boldsymbol'], '[+]': ['bm']}}

    }
  });
</script>




  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Alexander Ladd. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
