---
layout: base
title:  "Who's In Charge"
date:   2021-1-11 00:16:13 -0700
author: xander
categories: posts
permalink: /posts/whos_in_charge/
---

Well I am very busy so I'll keep this short and sweet but I was listening to [Alan Watts](https://www.youtube.com/watch?v=fLJincQZfas&t=748s&ab_channel=JoeyWizdom) and ever since I've turned it off this one thought has stuck with me. When we look at the whole world and all it's complex systems of flowing water, blossoming plants, diverse ecosystems, and even astronomical bodies and we wonder  **"who's in control around here?"** it's a bit scary of a question to ask. Well a hard studied scientist will go on about laws and physics so on and so forth, but I say to them -- "OK! who's in charge of physics?". On the other hand a religious person might say something to the tune of "that would be God!". And to that I say -- "OK! who's in charge of God?". No matter how you want to play this game, the system is self-controlling.

We go our whole lives thinking that an object does not move unless acted upon, but not all systems require external control. We need nouns to enact verbs upon other nouns to understand something. It goes against our strongest inuitions to think of processes as self-controlling. I think this is the story of the human...a high degree of spontaneous self-control that arose out of a self-controlled system. In my opinion, this reframes the chase for machine intelligence. Just take the bitter lesson by [Rich Sutton](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), where he states that one of the humans trying to imbue their intelligence into machines is not effective, instead we should use systems that leverage large scale computation. I agree but I think this is actually at odds with deep learning as it's understood today. I think this is because in deep learning is highly engineered. These systems do not grow and learn connections the way humans do. They do not start as simple and chaotic networks and then evolve into coherent parts, as the architecture must be decided beforehand. Plasticity is a dangerous thing for an artifical neural network because they were designed to evolve through one method only -- weights and backprop.

So, I think if we really want our systems to scale, we will need to take a backseat as intelligent designers. In my recent reviews of self-organizing systems I look at the kind of properties of SOTA self-engineering. With the exception of Conway's game of life, it isn't a very hot field. I think it would be very hard to invent a self-controlling system, simply by nature of "self-controlling". But as engineers I think it's time to step down from the pedestal. _Real computational beauty (and intelligence) is emergent, not designed_. I would add one disclaimer. Emergent machine intelligence is an philosopichal/ethical problem. One wrong turn in the generation of such model can have catastrophic echos. However, being that cellular automata are the pinnacle of human generated self controlling systems and people think neural networks might actually contain some kind of dangerous intelligence... I'm not too worried that emergent intelligence is getting out of hand any time soon. 
